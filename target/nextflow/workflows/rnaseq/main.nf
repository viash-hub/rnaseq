// rnaseq dev
// 
// This wrapper script is auto-generated by viash 0.8.0-RC5 and is thus a
// derivative work thereof. This software comes with ABSOLUTELY NO WARRANTY from
// Data Intuitive.
// 
// The component may contain files which fall under a different license. The
// authors of this component should specify the license in the header of such
// files, or include a separate license file detailing the licenses of all included
// files.

////////////////////////////
// VDSL3 helper functions //
////////////////////////////

// helper file: 'src/main/resources/io/viash/platforms/nextflow/arguments/_checkArgumentType.nf'
class UnexpectedArgumentTypeException extends Exception {
  String errorIdentifier
  String stage
  String plainName
  String expectedClass
  String foundClass
  
  // ${key ? " in module '$key'" : ""}${id ? " id '$id'" : ""}
  UnexpectedArgumentTypeException(String errorIdentifier, String stage, String plainName, String expectedClass, String foundClass) {
    super("Error${errorIdentifier ? " $errorIdentifier" : ""}:${stage ? " $stage" : "" } argument '${plainName}' has the wrong type. " +
      "Expected type: ${expectedClass}. Found type: ${foundClass}")
    this.errorIdentifier = errorIdentifier
    this.stage = stage
    this.plainName = plainName
    this.expectedClass = expectedClass
    this.foundClass = foundClass
  }
}

/**
  * Checks if the given value is of the expected type. If not, an exception is thrown.
  *
  * @param stage The stage of the argument (input or output)
  * @param par The parameter definition
  * @param value The value to check
  * @param errorIdentifier The identifier to use in the error message
  * @return The value, if it is of the expected type
  * @throws UnexpectedArgumentTypeException If the value is not of the expected type
*/
def _checkArgumentType(String stage, Map par, Object value, String errorIdentifier) {
  // expectedClass will only be != null if value is not of the expected type
  def expectedClass = null
  def foundClass = null
  
  // todo: split if need be
  
  if (!par.required && value == null) {
    expectedClass = null
  } else if (par.multiple) {
    if (value !instanceof Collection) {
      value = [value]
    }
    
    // split strings
    value = value.collectMany{ val ->
      if (val instanceof String) {
        // collect() to ensure that the result is a List and not simply an array
        val.split(par.multiple_sep).collect()
      } else {
        [val]
      }
    }

    // process globs
    if (par.type == "file" && par.direction == "input") {
      value = value.collect{ it instanceof String ? file(it, hidden: true) : it }.flatten()
    }

    // check types of elements in list
    try {
      value = value.collect { listVal ->
        _checkArgumentType(stage, par + [multiple: false], listVal, errorIdentifier)
      }
    } catch (UnexpectedArgumentTypeException e) {
      expectedClass = "List[${e.expectedClass}]"
      foundClass = "List[${e.foundClass}]"
    }
  } else if (par.type == "string") {
    // cast to string if need be
    if (value instanceof GString) {
      value = value.toString()
    }
    expectedClass = value instanceof String ? null : "String"
  } else if (par.type == "integer") {
    // cast to integer if need be
    if (value instanceof String) {
      try {
        value = value.toInteger()
      } catch (NumberFormatException e) {
        // do nothing
      }
    }
    if (value instanceof java.math.BigInteger) {
      value = value.intValue()
    }
    expectedClass = value instanceof Integer ? null : "Integer"
  } else if (par.type == "long") {
    // cast to long if need be
    if (value instanceof String) {
      try {
        value = value.toLong()
      } catch (NumberFormatException e) {
        // do nothing
      }
    }
    if (value instanceof Integer) {
      value = value.toLong()
    }
    expectedClass = value instanceof Long ? null : "Long"
  } else if (par.type == "double") {
    // cast to double if need be
    if (value instanceof String) {
      try {
        value = value.toDouble()
      } catch (NumberFormatException e) {
        // do nothing
      }
    }
    if (value instanceof java.math.BigDecimal) {
      value = value.doubleValue()
    }
    if (value instanceof Float) {
      value = value.toDouble()
    }
    expectedClass = value instanceof Double ? null : "Double"
  } else if (par.type == "boolean" | par.type == "boolean_true" | par.type == "boolean_false") {
    // cast to boolean if need ben
    if (value instanceof String) {
      def valueLower = value.toLowerCase()
      if (valueLower == "true") {
        value = true
      } else if (valueLower == "false") {
        value = false
      }
    }
    expectedClass = value instanceof Boolean ? null : "Boolean"
  } else if (par.type == "file" && (par.direction == "input" || stage == "output")) {
    // cast to path if need be
    if (value instanceof String) {
      value = file(value, hidden: true)
    }
    if (value instanceof File) {
      value = value.toPath()
    }
    expectedClass = value instanceof Path ? null : "Path"
  } else if (par.type == "file" && stage == "input" && par.direction == "output") {
    // cast to string if need be
    if (value instanceof GString) {
      value = value.toString()
    }
    expectedClass = value instanceof String ? null : "String"
  } else {
    // didn't find a match for par.type
    expectedClass = par.type
  }

  if (expectedClass != null) {
    if (foundClass == null) {
      foundClass = value.getClass().getName()
    }
    throw new UnexpectedArgumentTypeException(errorIdentifier, stage, par.plainName, expectedClass, foundClass)
  }
  
  return value
}
// helper file: 'src/main/resources/io/viash/platforms/nextflow/arguments/_processInputValues.nf'
Map _processInputValues(Map inputs, Map config, String id, String key) {
  if (!workflow.stubRun) {
    config.functionality.allArguments.each { arg ->
      if (arg.required) {
        assert inputs.containsKey(arg.plainName) && inputs.get(arg.plainName) != null : 
          "Error in module '${key}' id '${id}': required input argument '${arg.plainName}' is missing"
      }
    }

    inputs = inputs.collectEntries { name, value ->
      def par = config.functionality.allArguments.find { it.plainName == name && (it.direction == "input" || it.type == "file") }
      assert par != null : "Error in module '${key}' id '${id}': '${name}' is not a valid input argument"

      value = _checkArgumentType("input", par, value, "in module '$key' id '$id'")

      [ name, value ]
    }
  }
  return inputs
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/arguments/_processOutputValues.nf'
Map _processOutputValues(Map outputs, Map config, String id, String key) {
  if (!workflow.stubRun) {
    config.functionality.allArguments.each { arg ->
      if (arg.direction == "output" && arg.required) {
        assert outputs.containsKey(arg.plainName) && outputs.get(arg.plainName) != null : 
          "Error in module '${key}' id '${id}': required output argument '${arg.plainName}' is missing"
      }
    }

    outputs = outputs.collectEntries { name, value ->
      def par = config.functionality.allArguments.find { it.plainName == name && it.direction == "output" }
      assert par != null : "Error in module '${key}' id '${id}': '${name}' is not a valid output argument"
      
      value = _checkArgumentType("output", par, value, "in module '$key' id '$id'")
      
      [ name, value ]
    }
  }
  return outputs
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/IDChecker.nf'
class IDChecker {
  final def items = [] as Set

  @groovy.transform.WithWriteLock
  boolean observe(String item) {
    if (items.contains(item)) {
      return false
    } else {
      items << item
      return true
    }
  }

  @groovy.transform.WithReadLock
  boolean contains(String item) {
    return items.contains(item)
  }

  @groovy.transform.WithReadLock
  Set getItems() {
    return items.clone()
  }
}
// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/_checkUniqueIds.nf'

/**
 * Check if the ids are unique across parameter sets
 *
 * @param parameterSets a list of parameter sets.
 */
private void _checkUniqueIds(List<Tuple2<String, Map<String, Object>>> parameterSets) {
  def ppIds = parameterSets.collect{it[0]}
  assert ppIds.size() == ppIds.unique().size() : "All argument sets should have unique ids. Detected ids: $ppIds"
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/_getChild.nf'

// helper functions for reading params from file //
def _getChild(parent, child) {
  if (child.contains("://") || java.nio.file.Paths.get(child).isAbsolute()) {
    child
  } else {
    def parentAbsolute = java.nio.file.Paths.get(parent).toAbsolutePath().toString()
    parentAbsolute.replaceAll('/[^/]*$', "/") + child
  }
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/_parseParamList.nf'
/**
  * Figure out the param list format based on the file extension
  *
  * @param param_list A String containing the path to the parameter list file.
  *
  * @return A String containing the format of the parameter list file.
  */
def _paramListGuessFormat(param_list) {
  if (param_list !instanceof String) {
    "asis"
  } else if (param_list.endsWith(".csv")) {
    "csv"
  } else if (param_list.endsWith(".json") || param_list.endsWith(".jsn")) {
    "json"
  } else if (param_list.endsWith(".yaml") || param_list.endsWith(".yml")) {
    "yaml"
  } else {
    "yaml_blob"
  }
}


/**
  * Read the param list
  * 
  * @param param_list One of the following:
  *   - A String containing the path to the parameter list file (csv, json or yaml),
  *   - A yaml blob of a list of maps (yaml_blob),
  *   - Or a groovy list of maps (asis).
  * @param config A Map of the Viash configuration.
  * 
  * @return A List of Maps containing the parameters.
  */
def _parseParamList(param_list, Map config) {
  // first determine format by extension
  def paramListFormat = _paramListGuessFormat(param_list)

  def paramListPath = (paramListFormat != "asis" && paramListFormat != "yaml_blob") ?
    file(param_list, hidden: true) :
    null

  // get the correct parser function for the detected params_list format
  def paramSets = []
  if (paramListFormat == "asis") {
    paramSets = param_list
  } else if (paramListFormat == "yaml_blob") {
    paramSets = readYamlBlob(param_list)
  } else if (paramListFormat == "yaml") {
    paramSets = readYaml(paramListPath)
  } else if (paramListFormat == "json") {
    paramSets = readJson(paramListPath)
  } else if (paramListFormat == "csv") {
    paramSets = readCsv(paramListPath)
  } else {
    error "Format of provided --param_list not recognised.\n" +
    "Found: '$paramListFormat'.\n" +
    "Expected: a csv file, a json file, a yaml file,\n" +
    "a yaml blob or a groovy list of maps."
  }

  // data checks
  assert paramSets instanceof List: "--param_list should contain a list of maps"
  for (value in paramSets) {
    assert value instanceof Map: "--param_list should contain a list of maps"
  }

  // id is argument
  def idIsArgument = config.functionality.allArguments.any{it.plainName == "id"}

  // Reformat from List<Map> to List<Tuple2<String, Map>> by adding the ID as first element of a Tuple2
  paramSets = paramSets.collect({ data ->
    def id = data.id
    if (!idIsArgument) {
      data = data.findAll{k, v -> k != "id"}
    }
    [id, data]
  })

  // Split parameters with 'multiple: true'
  paramSets = paramSets.collect({ id, data ->
    data = _splitParams(data, config)
    [id, data]
  })
  
  // The paths of input files inside a param_list file may have been specified relatively to the
  // location of the param_list file. These paths must be made absolute.
  if (paramListPath) {
    paramSets = paramSets.collect({ id, data ->
      def new_data = data.collectEntries{ parName, parValue ->
        def par = config.functionality.allArguments.find{it.plainName == parName}
        if (par && par.type == "file" && par.direction == "input") {
          if (parValue instanceof Collection) {
            parValue = parValue.collect{path ->
              path instanceof String ? paramListPath.resolveSibling(path) : path
            }
          } else {
            parValue = parValue instanceof String ? paramListPath.resolveSibling(parValue) : parValue
          }
        }
        [parName, parValue]
      }
      [id, new_data]
    })
  }

  return paramSets
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/_splitParams.nf'
/**
 * Split parameters for arguments that accept multiple values using their separator
 *
 * @param paramList A Map containing parameters to split.
 * @param config A Map of the Viash configuration. This Map can be generated from the config file
 *               using the readConfig() function.
 *
 * @return A Map of parameters where the parameter values have been split into a list using
 *         their seperator.
 */
Map<String, Object> _splitParams(Map<String, Object> parValues, Map config){
  def parsedParamValues = parValues.collectEntries { parName, parValue ->
    def parameterSettings = config.functionality.allArguments.find({it.plainName == parName})

    if (!parameterSettings) {
      // if argument is not found, do not alter 
      return [parName, parValue]
    }
    if (parameterSettings.multiple) { // Check if parameter can accept multiple values
      if (parValue instanceof Collection) {
          parValue = parValue.collect{it instanceof String ? it.split(parameterSettings.multiple_sep) : it }
      } else if (parValue instanceof String) {
          parValue = parValue.split(parameterSettings.multiple_sep)
      } else if (parValue == null) {
          parValue = []
      } else {
          parValue = [ parValue ]
      }
      parValue = parValue.flatten()
    }
    // For all parameters check if multiple values are only passed for
    // arguments that allow it. Quietly simplify lists of length 1.
    if (!parameterSettings.multiple && parValue instanceof Collection) {
      assert parValue.size() == 1 : 
      "Error: argument ${parName} has too many values.\n" +
      "  Expected amount: 1. Found: ${parValue.size()}"
      parValue = parValue[0]
    }
    [parName, parValue]
  }
  return parsedParamValues
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/channelFromParams.nf'
/**
 * Parse nextflow parameters based on settings defined in a viash config.
 * Return a list of parameter sets, each parameter set corresponding to 
 * an event in a nextflow channel. The output from this function can be used
 * with Channel.fromList to create a nextflow channel with Vdsl3 formatted 
 * events.
 *
 * This function performs:
 *   - A filtering of the params which can be found in the config file.
 *   - Process the params_list argument which allows a user to to initialise 
 *     a Vsdl3 channel with multiple parameter sets. Possible formats are 
 *     csv, json, yaml, or simply a yaml_blob. A csv should have column names 
 *     which correspond to the different arguments of this pipeline. A json or a yaml
 *     file should be a list of maps, each of which has keys corresponding to the
 *     arguments of the pipeline. A yaml blob can also be passed directly as a parameter.
 *     When passing a csv, json or yaml, relative path names are relativized to the
 *     location of the parameter file.
 *   - Combine the parameter sets into a vdsl3 Channel.
 *
 * @param params Input parameters. Can optionaly contain a 'param_list' key that
 *               provides a list of arguments that can be split up into multiple events
 *               in the output channel possible formats of param_lists are: a csv file, 
 *               json file, a yaml file or a yaml blob. Each parameters set (event) must
 *               have a unique ID.
 * @param config A Map of the Viash configuration. This Map can be generated from the config file
 *               using the readConfig() function.
 * 
 * @return A list of parameters with the first element of the event being
 *         the event ID and the second element containing a map of the parsed parameters.
 */
 
private List<Tuple2<String, Map<String, Object>>> _paramsToParamSets(Map params, Map config){
  // todo: fetch key from run args
  def key_ = config.functionality.name
  
  /* parse regular parameters (not in param_list)  */
  /*************************************************/
  def globalParams = config.functionality.allArguments
    .findAll { params.containsKey(it.plainName) }
    .collectEntries { [ it.plainName, params[it.plainName] ] }
  def globalID = params.get("id", null)

  /* process params_list arguments */
  /*********************************/
  def paramList = params.containsKey("param_list") && params.param_list != null ?
    params.param_list : []
  // if (paramList instanceof String) {
  //   paramList = [paramList]
  // }
  // def paramSets = paramList.collectMany{ _parseParamList(it, config) }
  // TODO: be able to process param_list when it is a list of strings
  def paramSets = _parseParamList(paramList, config)
  if (paramSets.isEmpty()) {
    paramSets = [[null, [:]]]
  }

  /* combine arguments into channel */
  /**********************************/
  def processedParams = paramSets.indexed().collect{ index, tup ->
    // Process ID
    def id = tup[0] ?: globalID
  
    if (workflow.stubRun) {
      // if stub run, explicitly add an id if missing
      id = id ? id : "stub${index}"
    }
    assert id != null: "Each parameter set should have at least an 'id'"

    // Process params
    def parValues = globalParams + tup[1]
    // // Remove parameters which are null, if the default is also null
    // parValues = parValues.collectEntries{paramName, paramValue ->
    //   parameterSettings = config.functionality.allArguments.find({it.plainName == paramName})
    //   if ( paramValue != null || parameterSettings.get("default", null) != null ) {
    //     [paramName, paramValue]
    //   }
    // }
    parValues = parValues.collectEntries { name, value ->
      def par = config.functionality.allArguments.find { it.plainName == name && (it.direction == "input" || it.type == "file") }
      assert par != null : "Error in module '${key_}' id '${id}': '${name}' is not a valid input argument"

      if (par == null) {
        return [:]
      }
      value = _checkArgumentType("input", par, value, "in module '$key_' id '$id'")

      [ name, value ]
    }

    [id, parValues]
  }

  // Check if ids (first element of each list) is unique
  _checkUniqueIds(processedParams)
  return processedParams
}

/**
 * Parse nextflow parameters based on settings defined in a viash config 
 * and return a nextflow channel.
 * 
 * @param params Input parameters. Can optionaly contain a 'param_list' key that
 *               provides a list of arguments that can be split up into multiple events
 *               in the output channel possible formats of param_lists are: a csv file, 
 *               json file, a yaml file or a yaml blob. Each parameters set (event) must
 *               have a unique ID.
 * @param config A Map of the Viash configuration. This Map can be generated from the config file
 *               using the readConfig() function.
 * 
 * @return A nextflow Channel with events. Events are formatted as a tuple that contains 
 *         first contains the ID of the event and as second element holds a parameter map.
 *       
 *
 */
def channelFromParams(Map params, Map config) {
  processedParams = _paramsToParamSets(params, config)
  return Channel.fromList(processedParams)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/checkUniqueIds.nf'
def checkUniqueIds(Map args) {
  def stopOnError = args.stopOnError == null ? args.stopOnError : true

  def idChecker = new IDChecker()

  return filter { tup ->
    if (!idChecker.observe(tup[0])) {
      if (stopOnError) {
        error "Duplicate id: ${tup[0]}"
      } else {
        log.warn "Duplicate id: ${tup[0]}, removing duplicate entry"
        return false
      }
    }
    return true
  }
}
// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/preprocessInputs.nf'
// This helper file will be deprecated soon
preprocessInputsDeprecationWarningPrinted = false

def preprocessInputsDeprecationWarning() {
  if (!preprocessInputsDeprecationWarningPrinted) {
    preprocessInputsDeprecationWarningPrinted = true
    System.err.println("Warning: preprocessInputs() is deprecated and will be removed in Viash 0.9.0.")
  }
}

/**
 * Generate a nextflow Workflow that allows processing a channel of 
 * Vdsl3 formatted events and apply a Viash config to them:
 *    - Gather default parameters from the Viash config and make 
 *      sure that they are correctly formatted (see applyConfig method).
 *    - Format the input parameters (also using the applyConfig method).
 *    - Apply the default parameter to the input parameters.
 *    - Do some assertions:
 *        ~ Check if the event IDs in the channel are unique.
 * 
 * The events in the channel are formatted as tuples, with the 
 * first element of the tuples being a unique id of the parameter set, 
 * and the second element containg the the parameters themselves.
 * Optional extra elements of the tuples will be passed to the output as is.
 *
 * @param args A map that must contain a 'config' key that points
 *              to a parsed config (see readConfig()). Optionally, a
 *              'key' key can be provided which can be used to create a unique
 *              name for the workflow process.
 *
 * @return A workflow that allows processing a channel of Vdsl3 formatted events
 * and apply a Viash config to them.
 */
def preprocessInputs(Map args) {
  preprocessInputsDeprecationWarning()

  config = args.config
  assert config instanceof Map : 
    "Error in preprocessInputs: config must be a map. " +
    "Expected class: Map. Found: config.getClass() is ${config.getClass()}"
  def key_ = args.key ?: config.functionality.name

  // Get different parameter types (used throughout this function)
  def defaultArgs = config.functionality.allArguments
    .findAll { it.containsKey("default") }
    .collectEntries { [ it.plainName, it.default ] }

  map { tup ->
    def id = tup[0]
    def data = tup[1]
    def passthrough = tup.drop(2)

    def new_data = (defaultArgs + data).collectEntries { name, value ->
      def par = config.functionality.allArguments.find { it.plainName == name && (it.direction == "input" || it.type == "file") }
      
      if (par != null) {
        value = _checkArgumentType("input", par, value, "in module '$key_' id '$id'")
      }

      [ name, value ]
    }

    [ id, new_data ] + passthrough
  }
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/runComponents.nf'
/**
 * Run a list of components on a stream of data.
 * 
 * @param components: list of Viash VDSL3 modules to run
 * @param fromState: a closure, a map or a list of keys to extract from the input data.
 *   If a closure, it will be called with the id, the data and the component config.
 * @param toState: a closure, a map or a list of keys to extract from the output data
 *   If a closure, it will be called with the id, the output data, the old state and the component config.
 * @param filter: filter function to apply to the input.
 *   It will be called with the id, the data and the component config.
 * @param id: id to use for the output data
 *   If a closure, it will be called with the id, the data and the component config.
 * @param auto: auto options to pass to the components
 *
 * @return: a workflow that runs the components
 **/
def runComponents(Map args) {
  log.warn("runComponents is deprecated, use runEach instead")
  assert args.components: "runComponents should be passed a list of components to run"

  def components_ = args.components
  if (components_ !instanceof List) {
    components_ = [ components_ ]
  }
  assert components_.size() > 0: "pass at least one component to runComponents"

  def fromState_ = args.fromState
  def toState_ = args.toState
  def filter_ = args.filter
  def id_ = args.id

  workflow runComponentsWf {
    take: input_ch
    main:

    // generate one channel per method
    out_chs = components_.collect{ comp_ ->
      def comp_config = comp_.config

      filter_ch = filter_
        ? input_ch | filter{tup ->
          filter_(tup[0], tup[1], comp_config)
        }
        : input_ch
      id_ch = id_
        ? filter_ch | map{tup ->
          // def new_id = id_(tup[0], tup[1], comp_config)
          def new_id = tup[0]
          if (id_ instanceof String) {
            new_id = id_
          } else if (id_ instanceof Closure) {
            new_id = id_(new_id, tup[1], comp_config)
          }
          [new_id] + tup.drop(1)
        }
        : filter_ch
      data_ch = id_ch | map{tup ->
          def new_data = tup[1]
          if (fromState_ instanceof Map) {
            new_data = fromState_.collectEntries{ key0, key1 ->
              [key0, new_data[key1]]
            }
          } else if (fromState_ instanceof List) {
            new_data = fromState_.collectEntries{ key ->
              [key, new_data[key]]
            }
          } else if (fromState_ instanceof Closure) {
            new_data = fromState_(tup[0], new_data, comp_config)
          }
          tup.take(1) + [new_data] + tup.drop(1)
        }
      out_ch = data_ch
        | comp_.run(
          auto: (args.auto ?: [:]) + [simplifyInput: false, simplifyOutput: false]
        )
      post_ch = toState_
        ? out_ch | map{tup ->
          def output = tup[1]
          def old_state = tup[2]
          if (toState_ instanceof Map) {
            new_state = old_state + toState_.collectEntries{ key0, key1 ->
              [key0, output[key1]]
            }
          } else if (toState_ instanceof List) {
            new_state = old_state + toState_.collectEntries{ key ->
              [key, output[key]]
            }
          } else if (toState_ instanceof Closure) {
            new_state = toState_(tup[0], output, old_state, comp_config)
          }
          [tup[0], new_state] + tup.drop(3)
        }
        : out_ch
      
      post_ch
    }

    // mix all results
    output_ch =
      (out_chs.size == 1)
        ? out_chs[0]
        : out_chs[0].mix(*out_chs.drop(1))

    emit: output_ch
  }

  return runComponentsWf
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/runEach.nf'
/**
 * Run a list of components on a stream of data.
 * 
 * @param components: list of Viash VDSL3 modules to run
 * @param fromState: a closure, a map or a list of keys to extract from the input data.
 *   If a closure, it will be called with the id, the data and the component itself.
 * @param toState: a closure, a map or a list of keys to extract from the output data
 *   If a closure, it will be called with the id, the output data, the old state and the component itself.
 * @param filter: filter function to apply to the input.
 *   It will be called with the id, the data and the component itself.
 * @param id: id to use for the output data
 *   If a closure, it will be called with the id, the data and the component itself.
 * @param auto: auto options to pass to the components
 *
 * @return: a workflow that runs the components
 **/
def runEach(Map args) {
  assert args.components: "runEach should be passed a list of components to run"

  def components_ = args.components
  if (components_ !instanceof List) {
    components_ = [ components_ ]
  }
  assert components_.size() > 0: "pass at least one component to runEach"

  def fromState_ = args.fromState
  def toState_ = args.toState
  def filter_ = args.filter
  def id_ = args.id

  workflow runEachWf {
    take: input_ch
    main:

    // generate one channel per method
    out_chs = components_.collect{ comp_ ->
      filter_ch = filter_
        ? input_ch | filter{tup ->
          filter_(tup[0], tup[1], comp_)
        }
        : input_ch
      id_ch = id_
        ? filter_ch | map{tup ->
          // def new_id = id_(tup[0], tup[1], comp_)
          def new_id = tup[0]
          if (id_ instanceof String) {
            new_id = id_
          } else if (id_ instanceof Closure) {
            new_id = id_(new_id, tup[1], comp_)
          }
          [new_id] + tup.drop(1)
        }
        : filter_ch
      data_ch = id_ch | map{tup ->
          def new_data = tup[1]
          if (fromState_ instanceof Map) {
            new_data = fromState_.collectEntries{ key0, key1 ->
              [key0, new_data[key1]]
            }
          } else if (fromState_ instanceof List) {
            new_data = fromState_.collectEntries{ key ->
              [key, new_data[key]]
            }
          } else if (fromState_ instanceof Closure) {
            new_data = fromState_(tup[0], new_data, comp_)
          }
          tup.take(1) + [new_data] + tup.drop(1)
        }
      out_ch = data_ch
        | comp_.run(
          auto: (args.auto ?: [:]) + [simplifyInput: false, simplifyOutput: false]
        )
      post_ch = toState_
        ? out_ch | map{tup ->
          def output = tup[1]
          def old_state = tup[2]
          if (toState_ instanceof Map) {
            new_state = old_state + toState_.collectEntries{ key0, key1 ->
              [key0, output[key1]]
            }
          } else if (toState_ instanceof List) {
            new_state = old_state + toState_.collectEntries{ key ->
              [key, output[key]]
            }
          } else if (toState_ instanceof Closure) {
            new_state = toState_(tup[0], output, old_state, comp_)
          }
          [tup[0], new_state] + tup.drop(3)
        }
        : out_ch
      
      post_ch
    }

    // mix all results
    output_ch =
      (out_chs.size == 1)
        ? out_chs[0]
        : out_chs[0].mix(*out_chs.drop(1))

    emit: output_ch
  }

  return runEachWf
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/channel/safeJoin.nf'
/**
 * Join sourceChannel to targetChannel
 * 
 * This function joins the sourceChannel to the targetChannel. 
 * However, each id in the targetChannel must be present in the
 * sourceChannel. If _meta.join_id exists in the targetChannel, that is 
 * used as an id instead. If the id doesn't match any id in the sourceChannel,
 * an error is thrown.
 */

def safeJoin(targetChannel, sourceChannel, key) {
  def sourceIDs = new IDChecker()

  def sourceCheck = sourceChannel
    | map { tup ->
      sourceIDs.observe(tup[0])
      tup
    }
  def targetCheck = targetChannel
    | map { tup ->
      def id = tup[0]
      
      if (!sourceIDs.contains(id)) {
        error (
          "Error in module '${key}' when merging output with original state.\n" +
          "  Reason: output with id '${id}' could not be joined with source channel.\n" +
          "    If the IDs in the output channel differ from the input channel,\n" + 
          "    please set `tup[1]._meta.join_id to the original ID.\n" +
          "  Original IDs in input channel: ['${sourceIDs.getItems().join("', '")}'].\n" + 
          "  Unexpected ID in the output channel: '${id}'.\n" +
          "  Example input event: [\"id\", [input: file(...)]],\n" +
          "  Example output event: [\"newid\", [output: file(...), _meta: [join_id: \"id\"]]]"
        )
      }
      // TODO: add link to our documentation on how to fix this

      tup
    }
  
  sourceCheck.cross(targetChannel)
    | map{ left, right ->
      right + left.drop(1)
    }
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/config/_processArgument.nf'
def _processArgument(arg) {
  arg.multiple = arg.multiple != null ? arg.multiple : false
  arg.required = arg.required != null ? arg.required : false
  arg.direction = arg.direction != null ? arg.direction : "input"
  arg.multiple_sep = arg.multiple_sep != null ? arg.multiple_sep : ":"
  arg.plainName = arg.name.replaceAll("^-*", "")

  if (arg.type == "file") {
    arg.must_exist = arg.must_exist != null ? arg.must_exist : true
    arg.create_parent = arg.create_parent != null ? arg.create_parent : true
  }

  // add default values to output files which haven't already got a default
  if (arg.type == "file" && arg.direction == "output" && arg.default == null) {
    def mult = arg.multiple ? "_*" : ""
    def extSearch = ""
    if (arg.default != null) {
      extSearch = arg.default
    } else if (arg.example != null) {
      extSearch = arg.example
    }
    if (extSearch instanceof List) {
      extSearch = extSearch[0]
    }
    def extSearchResult = extSearch.find("\\.[^\\.]+\$")
    def ext = extSearchResult != null ? extSearchResult : ""
    arg.default = "\$id.\$key.${arg.plainName}${mult}${ext}"
    if (arg.multiple) {
      arg.default = [arg.default]
    }
  }

  if (!arg.multiple) {
    if (arg.default != null && arg.default instanceof List) {
      arg.default = arg.default[0]
    }
    if (arg.example != null && arg.example instanceof List) {
      arg.example = arg.example[0]
    }
  }

  if (arg.type == "boolean_true") {
    arg.default = false
  }
  if (arg.type == "boolean_false") {
    arg.default = true
  }

  arg
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/config/addGlobalParams.nf'
def addGlobalArguments(config) {
  def localConfig = [
    "functionality" : [
      "argument_groups": [
        [
          "name": "Nextflow input-output arguments",
          "description": "Input/output parameters for Nextflow itself. Please note that both publishDir and publish_dir are supported but at least one has to be configured.",
          "arguments" : [
            [
              'name': '--publish_dir',
              'required': true,
              'type': 'string',
              'description': 'Path to an output directory.',
              'example': 'output/',
              'multiple': false
            ],
            [
              'name': '--param_list',
              'required': false,
              'type': 'string',
              'description': '''Allows inputting multiple parameter sets to initialise a Nextflow channel. A `param_list` can either be a list of maps, a csv file, a json file, a yaml file, or simply a yaml blob.
              |
              |* A list of maps (as-is) where the keys of each map corresponds to the arguments of the pipeline. Example: in a `nextflow.config` file: `param_list: [ ['id': 'foo', 'input': 'foo.txt'], ['id': 'bar', 'input': 'bar.txt'] ]`.
              |* A csv file should have column names which correspond to the different arguments of this pipeline. Example: `--param_list data.csv` with columns `id,input`.
              |* A json or a yaml file should be a list of maps, each of which has keys corresponding to the arguments of the pipeline. Example: `--param_list data.json` with contents `[ {'id': 'foo', 'input': 'foo.txt'}, {'id': 'bar', 'input': 'bar.txt'} ]`.
              |* A yaml blob can also be passed directly as a string. Example: `--param_list "[ {'id': 'foo', 'input': 'foo.txt'}, {'id': 'bar', 'input': 'bar.txt'} ]"`.
              |
              |When passing a csv, json or yaml file, relative path names are relativized to the location of the parameter file. No relativation is performed when `param_list` is a list of maps (as-is) or a yaml blob.'''.stripMargin(),
              'example': 'my_params.yaml',
              'multiple': false,
              'hidden': true
            ]
            // TODO: allow multiple: true in param_list?
            // TODO: allow to specify a --param_list_regex to filter the param_list?
            // TODO: allow to specify a --param_list_from_state to remap entries in the param_list?
          ]
        ]
      ]
    ]
  ]

  return processConfig(_mergeMap(config, localConfig))
}

def _mergeMap(Map lhs, Map rhs) {
  return rhs.inject(lhs.clone()) { map, entry ->
    if (map[entry.key] instanceof Map && entry.value instanceof Map) {
      map[entry.key] = _mergeMap(map[entry.key], entry.value)
    } else if (map[entry.key] instanceof Collection && entry.value instanceof Collection) {
      map[entry.key] += entry.value
    } else {
      map[entry.key] = entry.value
    }
    return map
  }
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/config/generateHelp.nf'
def _generateArgumentHelp(param) {
  // alternatives are not supported
  // def names = param.alternatives ::: List(param.name)

  def unnamedProps = [
    ["required parameter", param.required],
    ["multiple values allowed", param.multiple],
    ["output", param.direction.toLowerCase() == "output"],
    ["file must exist", param.type == "file" && param.must_exist]
  ].findAll{it[1]}.collect{it[0]}
  
  def dflt = null
  if (param.default != null) {
    if (param.default instanceof List) {
      dflt = param.default.join(param.multiple_sep != null ? param.multiple_sep : ", ")
    } else {
      dflt = param.default.toString()
    }
  }
  def example = null
  if (param.example != null) {
    if (param.example instanceof List) {
      example = param.example.join(param.multiple_sep != null ? param.multiple_sep : ", ")
    } else {
      example = param.example.toString()
    }
  }
  def min = param.min?.toString()
  def max = param.max?.toString()

  def escapeChoice = { choice ->
    def s1 = choice.replaceAll("\\n", "\\\\n")
    def s2 = s1.replaceAll("\"", """\\\"""")
    s2.contains(",") || s2 != choice ? "\"" + s2 + "\"" : s2
  }
  def choices = param.choices == null ? 
    null : 
    "[ " + param.choices.collect{escapeChoice(it.toString())}.join(", ") + " ]"

  def namedPropsStr = [
    ["type", ([param.type] + unnamedProps).join(", ")],
    ["default", dflt],
    ["example", example],
    ["choices", choices],
    ["min", min],
    ["max", max]
  ]
    .findAll{it[1]}
    .collect{"\n        " + it[0] + ": " + it[1].replaceAll("\n", "\\n")}
    .join("")
  
  def descStr = param.description == null ?
    "" :
    _paragraphWrap("\n" + param.description.trim(), 80 - 8).join("\n        ")
  
  "\n    --" + param.plainName +
    namedPropsStr +
    descStr
}

// Based on Helper.generateHelp() in Helper.scala
def _generateHelp(config) {
  def fun = config.functionality

  // PART 1: NAME AND VERSION
  def nameStr = fun.name + 
    (fun.version == null ? "" : " " + fun.version)

  // PART 2: DESCRIPTION
  def descrStr = fun.description == null ? 
    "" :
    "\n\n" + _paragraphWrap(fun.description.trim(), 80).join("\n")

  // PART 3: Usage
  def usageStr = fun.usage == null ? 
    "" :
    "\n\nUsage:\n" + fun.usage.trim()

  // PART 4: Options
  def argGroupStrs = fun.allArgumentGroups.collect{argGroup ->
    def name = argGroup.name
    def descriptionStr = argGroup.description == null ?
      "" :
      "\n    " + _paragraphWrap(argGroup.description.trim(), 80-4).join("\n    ") + "\n"
    def arguments = argGroup.arguments.collect{arg -> 
      arg instanceof String ? fun.allArguments.find{it.plainName == arg} : arg
    }.findAll{it != null}
    def argumentStrs = arguments.collect{param -> _generateArgumentHelp(param)}
    
    "\n\n$name:" +
      descriptionStr +
      argumentStrs.join("\n")
  }

  // FINAL: combine
  def out = nameStr + 
    descrStr +
    usageStr + 
    argGroupStrs.join("")

  return out
}

// based on Format._paragraphWrap
def _paragraphWrap(str, maxLength) {
  def outLines = []
  str.split("\n").each{par ->
    def words = par.split("\\s").toList()

    def word = null
    def line = words.pop()
    while(!words.isEmpty()) {
      word = words.pop()
      if (line.length() + word.length() + 1 <= maxLength) {
        line = line + " " + word
      } else {
        outLines.add(line)
        line = word
      }
    }
    if (words.isEmpty()) {
      outLines.add(line)
    }
  }
  return outLines
}

def helpMessage(config) {
  if (params.containsKey("help") && params.help) {
    def mergedConfig = addGlobalArguments(config)
    def helpStr = _generateHelp(mergedConfig)
    println(helpStr)
    exit 0
  }
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/config/processConfig.nf'
def processConfig(config) {
  // set defaults for arguments
  config.functionality.arguments = 
    (config.functionality.arguments ?: []).collect{_processArgument(it)}

  // set defaults for argument_group arguments
  config.functionality.argument_groups =
    (config.functionality.argument_groups ?: []).collect{grp ->
      grp.arguments = (grp.arguments ?: []).collect{_processArgument(it)}
      grp
    }

  // create combined arguments list
  config.functionality.allArguments = 
    config.functionality.arguments +
    config.functionality.argument_groups.collectMany{it.arguments}

  // add missing argument groups (based on Functionality::allArgumentGroups())
  def argGroups = config.functionality.argument_groups
  if (argGroups.any{it.name.toLowerCase() == "arguments"}) {
    argGroups = argGroups.collect{ grp ->
      if (grp.name.toLowerCase() == "arguments") {
        grp = grp + [
          arguments: grp.arguments + config.functionality.arguments
        ]
      }
      grp
    }
  } else {
    argGroups = argGroups + [
      name: "Arguments",
      arguments: config.functionality.arguments
    ]
  }
  config.functionality.allArgumentGroups = argGroups

  config
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/config/readConfig.nf'

def readConfig(file) {
  def config = readYaml(file ?: moduleDir.resolve("config.vsh.yaml"))
  processConfig(config)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/functions/collectTraces.nf'
class CustomTraceObserver implements nextflow.trace.TraceObserver {
  List traces

  CustomTraceObserver(List traces) {
    this.traces = traces
  }

  @Override
  void onProcessComplete(nextflow.processor.TaskHandler handler, nextflow.trace.TraceRecord trace) {
    def trace2 = trace.store.clone()
    trace2.script = null
    traces.add(trace2)
  }

  @Override
  void onProcessCached(nextflow.processor.TaskHandler handler, nextflow.trace.TraceRecord trace) {
    def trace2 = trace.store.clone()
    trace2.script = null
    traces.add(trace2)
  }
}

def collectTraces() {
  def traces = Collections.synchronizedList([])

  // add custom trace observer which stores traces in the traces object
  session.observers.add(new CustomTraceObserver(traces))

  traces
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/functions/deepClone.nf'
/**
  * Performs a deep clone of the given object.
  * @param x an object
  */
def deepClone(x) {
  iterateMap(x, {it instanceof Cloneable ? it.clone() : it})
}
// helper file: 'src/main/resources/io/viash/platforms/nextflow/functions/getPublishDir.nf'
def getPublishDir() {
  return params.containsKey("publish_dir") ? params.publish_dir : 
    params.containsKey("publishDir") ? params.publishDir : 
    null
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/functions/getRootDir.nf'

// Recurse upwards until we find a '.build.yaml' file
def _findBuildYamlFile(path) {
  def child = path.resolve(".build.yaml")
  if (java.nio.file.Files.isDirectory(path) && java.nio.file.Files.exists(child)) {
    return child
  } else {
    def parent = path.getParent()
    if (parent == null) {
      return null
    } else {
      return _findBuildYamlFile(parent)
    }
  }
}

// get the root of the target folder
def getRootDir() {
  def dir = _findBuildYamlFile(moduleDir.normalize())
  assert dir != null: "Could not find .build.yaml in the folder structure"
  dir.getParent()
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/functions/iterateMap.nf'
/**
  * Recursively apply a function over the leaves of an object.
  * @param obj The object to iterate over.
  * @param fun The function to apply to each value.
  * @return The object with the function applied to each value.
  */
def iterateMap(obj, fun) {
  if (obj instanceof List && obj !instanceof String) {
    return obj.collect{item ->
      iterateMap(item, fun)
    }
  } else if (obj instanceof Map) {
    return obj.collectEntries{key, item ->
      [key.toString(), iterateMap(item, fun)]
    }
  } else {
    return fun(obj)
  }
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/functions/niceView.nf'
/**
  * A view for printing the event of each channel as a YAML blob.
  * This is useful for debugging.
  */
def niceView() {
  workflow niceViewWf {
    take: input
    main:
      output = input
        | view{toYamlBlob(it)}
    emit: output
  }
  return niceViewWf
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/readCsv.nf'

def readCsv(file_path) {
  def output = []
  def inputFile = file_path !instanceof Path ? file(file_path, hidden: true) : file_path

  // todo: allow escaped quotes in string
  // todo: allow single quotes?
  def splitRegex = java.util.regex.Pattern.compile(''',(?=(?:[^"]*"[^"]*")*[^"]*$)''')
  def removeQuote = java.util.regex.Pattern.compile('''"(.*)"''')

  def br = java.nio.file.Files.newBufferedReader(inputFile)

  def row = -1
  def header = null
  while (br.ready() && header == null) {
    def line = br.readLine()
    row++
    if (!line.startsWith("#")) {
      header = splitRegex.split(line, -1).collect{field ->
        m = removeQuote.matcher(field)
        m.find() ? m.replaceFirst('$1') : field
      }
    }
  }
  assert header != null: "CSV file should contain a header"

  while (br.ready()) {
    def line = br.readLine()
    row++
    if (line == null) {
      br.close()
      break
    }

    if (!line.startsWith("#")) {
      def predata = splitRegex.split(line, -1)
      def data = predata.collect{field ->
        if (field == "") {
          return null
        }
        m = removeQuote.matcher(field)
        if (m.find()) {
          return m.replaceFirst('$1')
        } else {
          return field
        }
      }
      assert header.size() == data.size(): "Row $row should contain the same number as fields as the header"
      
      def dataMap = [header, data].transpose().collectEntries().findAll{it.value != null}
      output.add(dataMap)
    }
  }

  output
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/readJson.nf'
def readJson(file_path) {
  def inputFile = file_path !instanceof Path ? file(file_path, hidden: true) : file_path
  def jsonSlurper = new groovy.json.JsonSlurper()
  jsonSlurper.parse(inputFile)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/readJsonBlob.nf'
def readJsonBlob(str) {
  def jsonSlurper = new groovy.json.JsonSlurper()
  jsonSlurper.parseText(str)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/readTaggedYaml.nf'
// Custom constructor to modify how certain objects are parsed from YAML
class CustomConstructor extends org.yaml.snakeyaml.constructor.Constructor {
  Path root

  class ConstructPath extends org.yaml.snakeyaml.constructor.AbstractConstruct {
    public Object construct(org.yaml.snakeyaml.nodes.Node node) {
      String filename = (String) constructScalar(node);
      if (root != null) {
        return root.resolve(filename);
      }
      return java.nio.file.Paths.get(filename);
    }
  }

  CustomConstructor(org.yaml.snakeyaml.LoaderOptions options, Path root) {
    super(options)
    this.root = root
    // Handling !file tag and parse it back to a File type
    this.yamlConstructors.put(new org.yaml.snakeyaml.nodes.Tag("!file"), new ConstructPath())
  }
}

def readTaggedYaml(Path path) {
  def options = new org.yaml.snakeyaml.LoaderOptions()
  def constructor = new CustomConstructor(options, path.getParent())
  def yaml = new org.yaml.snakeyaml.Yaml(constructor)
  return yaml.load(path.text)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/readYaml.nf'
def readYaml(file_path) {
  def inputFile = file_path !instanceof Path ? file(file_path, hidden: true) : file_path
  def yamlSlurper = new org.yaml.snakeyaml.Yaml()
  yamlSlurper.load(inputFile)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/readYamlBlob.nf'
def readYamlBlob(str) {
  def yamlSlurper = new org.yaml.snakeyaml.Yaml()
  yamlSlurper.load(str)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/toJsonBlob.nf'
String toJsonBlob(data) {
  return groovy.json.JsonOutput.toJson(data)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/toTaggedYamlBlob.nf'
// Custom representer to modify how certain objects are represented in YAML
class CustomRepresenter extends org.yaml.snakeyaml.representer.Representer {
  Path relativizer

  class RepresentPath implements org.yaml.snakeyaml.representer.Represent {
    public String getFileName(Object obj) {
      if (obj instanceof File) {
        obj = ((File) obj).toPath();
      }
      if (obj !instanceof Path) {
        throw new IllegalArgumentException("Object: " + obj + " is not a Path or File");
      }
      def path = (Path) obj;

      if (relativizer != null) {
        return relativizer.relativize(path).toString()
      } else {
        return path.toString()
      }
    }

    public org.yaml.snakeyaml.nodes.Node representData(Object data) {
      String filename = getFileName(data);
      def tag = new org.yaml.snakeyaml.nodes.Tag("!file");
      return representScalar(tag, filename);
    }
  }
  CustomRepresenter(org.yaml.snakeyaml.DumperOptions options, Path relativizer) {
    super(options)
    this.relativizer = relativizer
    this.representers.put(sun.nio.fs.UnixPath, new RepresentPath())
    this.representers.put(Path, new RepresentPath())
    this.representers.put(File, new RepresentPath())
  }
}

String toTaggedYamlBlob(data) {
  return toRelativeTaggedYamlBlob(data, null)
}
String toRelativeTaggedYamlBlob(data, Path relativizer) {
  def options = new org.yaml.snakeyaml.DumperOptions()
  options.setDefaultFlowStyle(org.yaml.snakeyaml.DumperOptions.FlowStyle.BLOCK)
  def representer = new CustomRepresenter(options, relativizer)
  def yaml = new org.yaml.snakeyaml.Yaml(representer, options)
  return yaml.dump(data)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/toYamlBlob.nf'
String toYamlBlob(data) {
  def options = new org.yaml.snakeyaml.DumperOptions()
  options.setDefaultFlowStyle(org.yaml.snakeyaml.DumperOptions.FlowStyle.BLOCK)
  options.setPrettyFlow(true)
  def yaml = new org.yaml.snakeyaml.Yaml(options)
  def cleanData = iterateMap(data, { it instanceof Path ? it.toString() : it })
  return yaml.dump(cleanData)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/writeJson.nf'
void writeJson(data, file) {
  assert data: "writeJson: data should not be null"
  assert file: "writeJson: file should not be null"
  file.write(toJsonBlob(data))
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/readwrite/writeYaml.nf'
void writeYaml(data, file) {
  assert data: "writeYaml: data should not be null"
  assert file: "writeYaml: file should not be null"
  file.write(toYamlBlob(data))
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/states/findStates.nf'
def findStates(Map params, Map config) {
  // TODO: do a deep clone of config
  def auto_config = deepClone(config)
  def auto_params = deepClone(params)

  auto_config = auto_config.clone()
  // override arguments
  auto_config.functionality.argument_groups = []
  auto_config.functionality.arguments = [
    [
      type: "string",
      name: "--id",
      description: "A dummy identifier",
      required: false
    ],
    [
      type: "file",
      name: "--input_states",
      example: "/path/to/input/directory/**/state.yaml",
      description: "Path to input directory containing the datasets to be integrated.",
      required: true,
      multiple: true,
      multiple_sep: ";"
    ],
    [
      type: "string",
      name: "--filter",
      example: "foo/.*/state.yaml",
      description: "Regex to filter state files by path.",
      required: false
    ],
    // to do: make this a yaml blob?
    [
      type: "string",
      name: "--rename_keys",
      example: ["newKey1:oldKey1", "newKey2:oldKey2"],
      description: "Rename keys in the detected input files. This is useful if the input files do not match the set of input arguments of the workflow.",
      required: false,
      multiple: true,
      multiple_sep: ","
    ],
    [
      type: "string",
      name: "--settings",
      example: '{"output_dataset": "dataset.h5ad", "k": 10}',
      description: "Global arguments as a JSON glob to be passed to all components.",
      required: false
    ]
  ]
  if (!(auto_params.containsKey("id"))) {
    auto_params["id"] = "auto"
  }

  // run auto config through processConfig once more
  auto_config = processConfig(auto_config)

  workflow findStatesWf {
    helpMessage(auto_config)

    output_ch = 
      channelFromParams(auto_params, auto_config)
        | flatMap { autoId, args ->

          def globalSettings = args.settings ? readYamlBlob(args.settings) : [:]

          // look for state files in input dir
          def stateFiles = args.input_states

          // filter state files by regex
          if (args.filter) {
            stateFiles = stateFiles.findAll{ stateFile ->
              def stateFileStr = stateFile.toString()
              def matcher = stateFileStr =~ args.filter
              matcher.matches()}
          }

          // read in states
          def states = stateFiles.collect { stateFile ->
            def state_ = readTaggedYaml(stateFile)
            [state_.id, state_]
          }

          // construct renameMap
          if (args.rename_keys) {
            def renameMap = args.rename_keys.collectEntries{renameString ->
              def split = renameString.split(":")
              assert split.size() == 2: "Argument 'rename_keys' should be of the form 'newKey:oldKey,newKey:oldKey'"
              split
            }

            // rename keys in state, only let states through which have all keys
            // also add global settings
            states = states.collectMany{id, state ->
              def newState = [:]

              for (key in renameMap.keySet()) {
                def origKey = renameMap[key]
                if (!(state.containsKey(origKey))) {
                  return []
                }
                newState[key] = state[origKey]
              }

              [[id, globalSettings + newState]]
            }
          }

          states
        }
    emit:
    output_ch
  }

  return findStatesWf
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/states/joinStates.nf'
def joinStates(Closure apply_) {
  workflow joinStatesWf {
    take: input_ch
    main:
    output_ch = input_ch
      | toSortedList
      | filter{ it.size() > 0 }
      | map{ tups ->
        def ids = tups.collect{it[0]}
        def states = tups.collect{it[1]}
        apply_(ids, states)
      }

    emit: output_ch
  }
  return joinStatesWf
}
// helper file: 'src/main/resources/io/viash/platforms/nextflow/states/publishStates.nf'
def collectFiles(obj) {
  if (obj instanceof java.io.File || obj instanceof Path)  {
    return [obj]
  } else if (obj instanceof List && obj !instanceof String) {
    return obj.collectMany{item ->
      collectFiles(item)
    }
  } else if (obj instanceof Map) {
    return obj.collectMany{key, item ->
      collectFiles(item)
    }
  } else {
    return []
  }
}

/**
 * Recurse through a state and collect all input files and their target output filenames.
 * @param obj The state to recurse through.
 * @param prefix The prefix to prepend to the output filenames.
 */
def collectInputOutputPaths(obj, prefix) {
  if (obj instanceof File || obj instanceof Path)  {
    def path = obj instanceof Path ? obj : obj.toPath()
    def ext = path.getFileName().toString().find("\\.[^\\.]+\$") ?: ""
    def newFilename = prefix + ext
    return [[obj, newFilename]]
  } else if (obj instanceof List && obj !instanceof String) {
    return obj.withIndex().collectMany{item, ix ->
      collectInputOutputPaths(item, prefix + "_" + ix)
    }
  } else if (obj instanceof Map) {
    return obj.collectMany{key, item ->
      collectInputOutputPaths(item, prefix + "." + key)
    }
  } else {
    return []
  }
}

def publishStates(Map args) {
  def key_ = args.get("key")
  def yamlTemplate_ = args.get("output_state", args.get("outputState", '$id.$key.state.yaml'))

  assert key_ != null : "publishStates: key must be specified"
  
  workflow publishStatesWf {
    take: input_ch
    main:
      input_ch
        | map { tup ->
          def id_ = tup[0]
          def state_ = tup[1]

          // the input files and the target output filenames
          def inputOutputFiles_ = collectInputOutputPaths(state_, id_ + "." + key_).transpose()
          def inputFiles_ = inputOutputFiles_[0]
          def outputFiles_ = inputOutputFiles_[1]

          def yamlFilename = yamlTemplate_
            .replaceAll('\\$id', id_)
            .replaceAll('\\$key', key_)

            // TODO: do the pathnames in state_ match up with the outputFiles_?

          // convert state to yaml blob
          def yamlBlob_ = toRelativeTaggedYamlBlob([id: id_] + state_, java.nio.file.Paths.get(yamlFilename))

          [id_, yamlBlob_, yamlFilename, inputFiles_, outputFiles_]
        }
        | publishStatesProc
    emit: input_ch
  }
  return publishStatesWf
}
process publishStatesProc {
  // todo: check publishpath?
  publishDir path: "${getPublishDir()}/", mode: "copy"
  tag "$id"
  input:
    tuple val(id), val(yamlBlob), val(yamlFile), path(inputFiles, stageAs: "_inputfile?/*"), val(outputFiles)
  output:
    tuple val(id), path{[yamlFile] + outputFiles}
  script:
  def copyCommands = [
    inputFiles instanceof List ? inputFiles : [inputFiles],
    outputFiles instanceof List ? outputFiles : [outputFiles]
  ]
    .transpose()
    .collectMany{infile, outfile ->
      if (infile.toString() != outfile.toString()) {
        [
          "[ -d \"\$(dirname '${outfile.toString()}')\" ] || mkdir -p \"\$(dirname '${outfile.toString()}')\"",
          "cp -r '${infile.toString()}' '${outfile.toString()}'"
        ]
      } else {
        // no need to copy if infile is the same as outfile
        []
      }
    }
  """
  mkdir -p "\$(dirname '${yamlFile}')"
  echo "Storing state as yaml"
  echo '${yamlBlob}' > '${yamlFile}'
  echo "Copying output files to destination folder"
  ${copyCommands.join("\n  ")}
  """
}


// this assumes that the state contains no other values other than those specified in the config
def publishStatesByConfig(Map args) {
  def config = args.get("config")
  assert config != null : "publishStatesByConfig: config must be specified"

  def key_ = args.get("key", config.functionality.name)
  assert key_ != null : "publishStatesByConfig: key must be specified"
  
  workflow publishStatesSimpleWf {
    take: input_ch
    main:
      input_ch
        | map { tup ->
          def id_ = tup[0]
          def state_ = tup[1] // e.g. [output: new File("myoutput.h5ad"), k: 10]
          def origState_ = tup[2] // e.g. [output: '$id.$key.foo.h5ad']

          // TODO: allow overriding the state.yaml template
          // TODO TODO: if auto.publish == "state", add output_state as an argument
          def yamlTemplate = params.containsKey("output_state") ? params.output_state : '$id.$key.state.yaml'
          def yamlFilename = yamlTemplate
            .replaceAll('\\$id', id_)
            .replaceAll('\\$key', key_)
          def yamlDir = java.nio.file.Paths.get(yamlFilename).getParent()

          // the processed state is a list of [key, value, srcPath, destPath] tuples, where
          //   - key, value is part of the state to be saved to disk
          //   - srcPath and destPath are lists of files to be copied from src to dest
          def processedState =
            config.functionality.allArguments
              .findAll { it.direction == "output" }
              .collectMany { par ->
                def plainName_ = par.plainName
                // if the state does not contain the key, it's an
                // optional argument for which the component did 
                // not generate any output
                if (!state_.containsKey(plainName_)) {
                  return []
                }
                def value = state_[plainName_]
                // if the parameter is not a file, it should be stored
                // in the state as-is, but is not something that needs 
                // to be copied from the source path to the dest path
                if (par.type != "file") {
                  return [[key: plainName_, value: value, srcPath: [], destPath: []]]
                }
                // if the orig state does not contain this filename,
                // it's an optional argument for which the user specified
                // that it should not be returned as a state
                if (!origState_.containsKey(plainName_)) {
                  return []
                }
                def filenameTemplate = origState_[plainName_]
                // if the pararameter is multiple: true, fetch the template
                if (par.multiple && filenameTemplate instanceof List) {
                  filenameTemplate = filenameTemplate[0]
                }
                // instantiate the template
                filename = filenameTemplate
                  .replaceAll('\\$id', id_)
                  .replaceAll('\\$key', key_)
                if (par.multiple) {
                  // if the parameter is multiple: true, the filename
                  // should contain a wildcard '*' that is replaced with
                  // the index of the file
                  assert filename.contains("*") : "Module '${key_}' id '${id_}': Multiple output files specified, but no wildcard '*' in the filename: ${filename}"
                  def outputPerFile = value.withIndex().collect{ val, ix ->
                    def value_ = java.nio.file.Paths.get(filename.replace("*", ix.toString()))
                    // if id contains a slash
                    if (yamlDir != null) {
                      value_ = yamlDir.relativize(value_)
                    }
                    def srcPath = val instanceof File ? val.toPath() : val
                    [value: value_, srcPath: srcPath, destPath: destPath]
                  }
                  def transposedOutputs = ["value", "srcPath", "destPath"].collectEntries{ key -> 
                    [key, outputPerFile.collect{dic -> dic[key]}]
                  }
                  return [[key: plainName_] + transposedOutputs]
                } else {
                  def value_ = java.nio.file.Paths.get(filename)
                  // if id contains a slash
                  if (yamlDir != null) {
                    value_ = yamlDir.relativize(value_)
                  }
                  def srcPath = value instanceof File ? value.toPath() : value
                  return [[key: plainName_, value: value_, srcPath: [srcPath], destPath: [filename]]]
                }
              }
          
          def updatedState_ = processedState.collectEntries{[it.key, it.value]}
          def inputFiles_ = processedState.collectMany{it.srcPath}
          def outputFiles_ = processedState.collectMany{it.destPath}
          
          // convert state to yaml blob
          def yamlBlob_ = toTaggedYamlBlob([id: id_] + updatedState_)

          [id_, yamlBlob_, yamlFilename, inputFiles_, outputFiles_]
        }
        | publishStatesProc
    emit: input_ch
  }
  return publishStatesSimpleWf
}
// helper file: 'src/main/resources/io/viash/platforms/nextflow/states/setState.nf'
def setState(fun) {
  assert fun instanceof Closure || fun instanceof Map || fun instanceof List :
    "Error in setState: Expected process argument to be a Closure, a Map, or a List. Found: class ${fun.getClass()}"

  // if fun is a List, convert to map
  if (fun instanceof List) {
    // check whether fun is a list[string]
    assert fun.every{it instanceof CharSequence} : "Error in setState: argument is a List, but not all elements are Strings"
    fun = fun.collectEntries{[it, it]}
  }

  // if fun is a map, convert to closure
  if (fun instanceof Map) {
    // check whether fun is a map[string, string]
    assert fun.values().every{it instanceof CharSequence} : "Error in setState: argument is a Map, but not all values are Strings"
    assert fun.keySet().every{it instanceof CharSequence} : "Error in setState: argument is a Map, but not all keys are Strings"
    def funMap = fun.clone()
    // turn the map into a closure to be used later on
    fun = { id_, state_ ->
      assert state_ instanceof Map : "Error in setState: the state is not a Map"
      funMap.collectMany{newkey, origkey ->
        if (state_.containsKey(origkey)) {
          [[newkey, state_[origkey]]]
        } else {
          []
        }
      }.collectEntries()
    }
  }

  map { tup ->
    def id = tup[0]
    def state = tup[1]
    def unfilteredState = fun(id, state)
    def newState = unfilteredState.findAll{key, val -> val != null}
    [id, newState] + tup.drop(2)
  }
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/workflowFactory/processAuto.nf'
// TODO: unit test processAuto
def processAuto(Map auto) {
  // remove null values
  auto = auto.findAll{k, v -> v != null}

  // check for unexpected keys
  def expectedKeys = ["simplifyInput", "simplifyOutput", "transcript", "publish"]
  def unexpectedKeys = auto.keySet() - expectedKeys
  assert unexpectedKeys.isEmpty(), "unexpected keys in auto: '${unexpectedKeys.join("', '")}'"

  // check auto.simplifyInput
  assert auto.simplifyInput instanceof Boolean, "auto.simplifyInput must be a boolean"

  // check auto.simplifyOutput
  assert auto.simplifyOutput instanceof Boolean, "auto.simplifyOutput must be a boolean"

  // check auto.transcript
  assert auto.transcript instanceof Boolean, "auto.transcript must be a boolean"

  // check auto.publish
  assert auto.publish instanceof Boolean || auto.publish == "state", "auto.publish must be a boolean or 'state'"

  return auto.subMap(expectedKeys)
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/workflowFactory/processDirectives.nf'
def assertMapKeys(map, expectedKeys, requiredKeys, mapName) {
  assert map instanceof Map : "Expected argument '$mapName' to be a Map. Found: class ${map.getClass()}"
  map.forEach { key, val -> 
    assert key in expectedKeys : "Unexpected key '$key' in ${mapName ? mapName + " " : ""}map"
  }
  requiredKeys.forEach { requiredKey -> 
    assert map.containsKey(requiredKey) : "Missing required key '$key' in ${mapName ? mapName + " " : ""}map"
  }
}

// TODO: unit test processDirectives
def processDirectives(Map drctv) {
  // remove null values
  drctv = drctv.findAll{k, v -> v != null}

  // check for unexpected keys
  def expectedKeys = [
    "accelerator", "afterScript", "beforeScript", "cache", "conda", "container", "containerOptions", "cpus", "disk", "echo", "errorStrategy", "executor", "machineType", "maxErrors", "maxForks", "maxRetries", "memory", "module", "penv", "pod", "publishDir", "queue", "label", "scratch", "storeDir", "stageInMode", "stageOutMode", "tag", "time"
  ]
  def unexpectedKeys = drctv.keySet() - expectedKeys
  assert unexpectedKeys.isEmpty() : "Unexpected keys in process directive: '${unexpectedKeys.join("', '")}'"

  /* DIRECTIVE accelerator
    accepted examples:
    - [ limit: 4, type: "nvidia-tesla-k80" ]
  */
  if (drctv.containsKey("accelerator")) {
    assertMapKeys(drctv["accelerator"], ["type", "limit", "request", "runtime"], [], "accelerator")
  }

  /* DIRECTIVE afterScript
    accepted examples:
    - "source /cluster/bin/cleanup"
  */
  if (drctv.containsKey("afterScript")) {
    assert drctv["afterScript"] instanceof CharSequence
  }

  /* DIRECTIVE beforeScript
    accepted examples:
    - "source /cluster/bin/setup"
  */
  if (drctv.containsKey("beforeScript")) {
    assert drctv["beforeScript"] instanceof CharSequence
  }

  /* DIRECTIVE cache
    accepted examples:
    - true
    - false
    - "deep"
    - "lenient"
  */
  if (drctv.containsKey("cache")) {
    assert drctv["cache"] instanceof CharSequence || drctv["cache"] instanceof Boolean
    if (drctv["cache"] instanceof CharSequence) {
      assert drctv["cache"] in ["deep", "lenient"] : "Unexpected value for cache"
    }
  }

  /* DIRECTIVE conda
    accepted examples:
    - "bwa=0.7.15"
    - "bwa=0.7.15 fastqc=0.11.5"
    - ["bwa=0.7.15", "fastqc=0.11.5"]
  */
  if (drctv.containsKey("conda")) {
    if (drctv["conda"] instanceof List) {
      drctv["conda"] = drctv["conda"].join(" ")
    }
    assert drctv["conda"] instanceof CharSequence
  }

  /* DIRECTIVE container
    accepted examples:
    - "foo/bar:tag"
    - [ registry: "reg", image: "im", tag: "ta" ]
      is transformed to "reg/im:ta"
    - [ image: "im" ] 
      is transformed to "im:latest"
  */
  if (drctv.containsKey("container")) {
    assert drctv["container"] instanceof Map || drctv["container"] instanceof CharSequence
    if (drctv["container"] instanceof Map) {
      def m = drctv["container"]
      assertMapKeys(m, [ "registry", "image", "tag" ], ["image"], "container")
      def part1 = 
        System.getenv('OVERRIDE_CONTAINER_REGISTRY') ? System.getenv('OVERRIDE_CONTAINER_REGISTRY') + "/" : 
        params.containsKey("override_container_registry") ? params["override_container_registry"] + "/" : // todo: remove?
        m.registry ? m.registry + "/" : 
        ""
      def part2 = m.image
      def part3 = m.tag ? ":" + m.tag : ":latest"
      drctv["container"] = part1 + part2 + part3
    }
  }

  /* DIRECTIVE containerOptions
    accepted examples:
    - "--foo bar"
    - ["--foo bar", "-f b"]
  */
  if (drctv.containsKey("containerOptions")) {
    if (drctv["containerOptions"] instanceof List) {
      drctv["containerOptions"] = drctv["containerOptions"].join(" ")
    }
    assert drctv["containerOptions"] instanceof CharSequence
  }

  /* DIRECTIVE cpus
    accepted examples:
    - 1
    - 10
  */
  if (drctv.containsKey("cpus")) {
    assert drctv["cpus"] instanceof Integer
  }

  /* DIRECTIVE disk
    accepted examples:
    - "1 GB"
    - "2TB"
    - "3.2KB"
    - "10.B"
  */
  if (drctv.containsKey("disk")) {
    assert drctv["disk"] instanceof CharSequence
    // assert drctv["disk"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
    // ^ does not allow closures
  }

  /* DIRECTIVE echo
    accepted examples:
    - true
    - false
  */
  if (drctv.containsKey("echo")) {
    assert drctv["echo"] instanceof Boolean
  }

  /* DIRECTIVE errorStrategy
    accepted examples:
    - "terminate"
    - "finish"
  */
  if (drctv.containsKey("errorStrategy")) {
    assert drctv["errorStrategy"] instanceof CharSequence
    assert drctv["errorStrategy"] in ["terminate", "finish", "ignore", "retry"] : "Unexpected value for errorStrategy"
  }

  /* DIRECTIVE executor
    accepted examples:
    - "local"
    - "sge"
  */
  if (drctv.containsKey("executor")) {
    assert drctv["executor"] instanceof CharSequence
    assert drctv["executor"] in ["local", "sge", "uge", "lsf", "slurm", "pbs", "pbspro", "moab", "condor", "nqsii", "ignite", "k8s", "awsbatch", "google-pipelines"] : "Unexpected value for executor"
  }

  /* DIRECTIVE machineType
    accepted examples:
    - "n1-highmem-8"
  */
  if (drctv.containsKey("machineType")) {
    assert drctv["machineType"] instanceof CharSequence
  }

  /* DIRECTIVE maxErrors
    accepted examples:
    - 1
    - 3
  */
  if (drctv.containsKey("maxErrors")) {
    assert drctv["maxErrors"] instanceof Integer
  }

  /* DIRECTIVE maxForks
    accepted examples:
    - 1
    - 3
  */
  if (drctv.containsKey("maxForks")) {
    assert drctv["maxForks"] instanceof Integer
  }

  /* DIRECTIVE maxRetries
    accepted examples:
    - 1
    - 3
  */
  if (drctv.containsKey("maxRetries")) {
    assert drctv["maxRetries"] instanceof Integer
  }

  /* DIRECTIVE memory
    accepted examples:
    - "1 GB"
    - "2TB"
    - "3.2KB"
    - "10.B"
  */
  if (drctv.containsKey("memory")) {
    assert drctv["memory"] instanceof CharSequence
    // assert drctv["memory"].matches("[0-9]+(\\.[0-9]*)? *[KMGTPEZY]?B")
    // ^ does not allow closures
  }

  /* DIRECTIVE module
    accepted examples:
    - "ncbi-blast/2.2.27"
    - "ncbi-blast/2.2.27:t_coffee/10.0"
    - ["ncbi-blast/2.2.27", "t_coffee/10.0"]
  */
  if (drctv.containsKey("module")) {
    if (drctv["module"] instanceof List) {
      drctv["module"] = drctv["module"].join(":")
    }
    assert drctv["module"] instanceof CharSequence
  }

  /* DIRECTIVE penv
    accepted examples:
    - "smp"
  */
  if (drctv.containsKey("penv")) {
    assert drctv["penv"] instanceof CharSequence
  }

  /* DIRECTIVE pod
    accepted examples:
    - [ label: "key", value: "val" ]
    - [ annotation: "key", value: "val" ]
    - [ env: "key", value: "val" ]
    - [ [label: "l", value: "v"], [env: "e", value: "v"]]
  */
  if (drctv.containsKey("pod")) {
    if (drctv["pod"] instanceof Map) {
      drctv["pod"] = [ drctv["pod"] ]
    }
    assert drctv["pod"] instanceof List
    drctv["pod"].forEach { pod ->
      assert pod instanceof Map
      // TODO: should more checks be added?
      // See https://www.nextflow.io/docs/latest/process.html?highlight=directives#pod
      // e.g. does it contain 'label' and 'value', or 'annotation' and 'value', or ...?
    }
  }

  /* DIRECTIVE publishDir
    accepted examples:
    - []
    - [ [ path: "foo", enabled: true ], [ path: "bar", enabled: false ] ]
    - "/path/to/dir" 
      is transformed to [[ path: "/path/to/dir" ]]
    - [ path: "/path/to/dir", mode: "cache" ]
      is transformed to [[ path: "/path/to/dir", mode: "cache" ]]
  */
  // TODO: should we also look at params["publishDir"]?
  if (drctv.containsKey("publishDir")) {
    def pblsh = drctv["publishDir"]
    
    // check different options
    assert pblsh instanceof List || pblsh instanceof Map || pblsh instanceof CharSequence
    
    // turn into list if not already so
    // for some reason, 'if (!pblsh instanceof List) pblsh = [ pblsh ]' doesn't work.
    pblsh = pblsh instanceof List ? pblsh : [ pblsh ]

    // check elements of publishDir
    pblsh = pblsh.collect{ elem ->
      // turn into map if not already so
      elem = elem instanceof CharSequence ? [ path: elem ] : elem

      // check types and keys
      assert elem instanceof Map : "Expected publish argument '$elem' to be a String or a Map. Found: class ${elem.getClass()}"
      assertMapKeys(elem, [ "path", "mode", "overwrite", "pattern", "saveAs", "enabled" ], ["path"], "publishDir")

      // check elements in map
      assert elem.containsKey("path")
      assert elem["path"] instanceof CharSequence
      if (elem.containsKey("mode")) {
        assert elem["mode"] instanceof CharSequence
        assert elem["mode"] in [ "symlink", "rellink", "link", "copy", "copyNoFollow", "move" ]
      }
      if (elem.containsKey("overwrite")) {
        assert elem["overwrite"] instanceof Boolean
      }
      if (elem.containsKey("pattern")) {
        assert elem["pattern"] instanceof CharSequence
      }
      if (elem.containsKey("saveAs")) {
        assert elem["saveAs"] instanceof CharSequence //: "saveAs as a Closure is currently not supported. Surround your closure with single quotes to get the desired effect. Example: '\{ foo \}'"
      }
      if (elem.containsKey("enabled")) {
        assert elem["enabled"] instanceof Boolean
      }

      // return final result
      elem
    }
    // store final directive
    drctv["publishDir"] = pblsh
  }

  /* DIRECTIVE queue
    accepted examples:
    - "long"
    - "short,long"
    - ["short", "long"]
  */
  if (drctv.containsKey("queue")) {
    if (drctv["queue"] instanceof List) {
      drctv["queue"] = drctv["queue"].join(",")
    }
    assert drctv["queue"] instanceof CharSequence
  }

  /* DIRECTIVE label
    accepted examples:
    - "big_mem"
    - "big_cpu"
    - ["big_mem", "big_cpu"]
  */
  if (drctv.containsKey("label")) {
    if (drctv["label"] instanceof CharSequence) {
      drctv["label"] = [ drctv["label"] ]
    }
    assert drctv["label"] instanceof List
    drctv["label"].forEach { label ->
      assert label instanceof CharSequence
      // assert label.matches("[a-zA-Z0-9]([a-zA-Z0-9_]*[a-zA-Z0-9])?")
      // ^ does not allow closures
    }
  }

  /* DIRECTIVE scratch
    accepted examples:
    - true
    - "/path/to/scratch"
    - '$MY_PATH_TO_SCRATCH'
    - "ram-disk"
  */
  if (drctv.containsKey("scratch")) {
    assert drctv["scratch"] == true || drctv["scratch"] instanceof CharSequence
  }

  /* DIRECTIVE storeDir
    accepted examples:
    - "/path/to/storeDir"
  */
  if (drctv.containsKey("storeDir")) {
    assert drctv["storeDir"] instanceof CharSequence
  }

  /* DIRECTIVE stageInMode
    accepted examples:
    - "copy"
    - "link"
  */
  if (drctv.containsKey("stageInMode")) {
    assert drctv["stageInMode"] instanceof CharSequence
    assert drctv["stageInMode"] in ["copy", "link", "symlink", "rellink"]
  }

  /* DIRECTIVE stageOutMode
    accepted examples:
    - "copy"
    - "link"
  */
  if (drctv.containsKey("stageOutMode")) {
    assert drctv["stageOutMode"] instanceof CharSequence
    assert drctv["stageOutMode"] in ["copy", "move", "rsync"]
  }

  /* DIRECTIVE tag
    accepted examples:
    - "foo"
    - '$id'
  */
  if (drctv.containsKey("tag")) {
    assert drctv["tag"] instanceof CharSequence
  }

  /* DIRECTIVE time
    accepted examples:
    - "1h"
    - "2days"
    - "1day 6hours 3minutes 30seconds"
  */
  if (drctv.containsKey("time")) {
    assert drctv["time"] instanceof CharSequence
    // todo: validation regex?
  }

  return drctv
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/workflowFactory/processWorkflowArgs.nf'
def processWorkflowArgs(Map args, Map defaultWfArgs, Map meta) {
  // override defaults with args
  def workflowArgs = defaultWfArgs + args

  // check whether 'key' exists
  assert workflowArgs.containsKey("key") : "Error in module '${meta.config.functionality.name}': key is a required argument"

  // if 'key' is a closure, apply it to the original key
  if (workflowArgs["key"] instanceof Closure) {
    workflowArgs["key"] = workflowArgs["key"](meta.config.functionality.name)
  }
  def key = workflowArgs["key"]
  assert key instanceof CharSequence : "Expected process argument 'key' to be a String. Found: class ${key.getClass()}"
  assert key ==~ /^[a-zA-Z_]\w*$/ : "Error in module '$key': Expected process argument 'key' to consist of only letters, digits or underscores. Found: ${key}"

  // check for any unexpected keys
  def expectedKeys = ["key", "directives", "auto", "map", "mapId", "mapData", "mapPassthrough", "filter", "runIf", "fromState", "toState", "args", "renameKeys", "debug"]
  def unexpectedKeys = workflowArgs.keySet() - expectedKeys
  assert unexpectedKeys.isEmpty() : "Error in module '$key': unexpected arguments to the '.run()' function: '${unexpectedKeys.join("', '")}'"

  // check whether directives exists and apply defaults
  assert workflowArgs.containsKey("directives") : "Error in module '$key': directives is a required argument"
  assert workflowArgs["directives"] instanceof Map : "Error in module '$key': Expected process argument 'directives' to be a Map. Found: class ${workflowArgs['directives'].getClass()}"
  workflowArgs["directives"] = processDirectives(defaultWfArgs.directives + workflowArgs["directives"])

  // check whether directives exists and apply defaults
  assert workflowArgs.containsKey("auto") : "Error in module '$key': auto is a required argument"
  assert workflowArgs["auto"] instanceof Map : "Error in module '$key': Expected process argument 'auto' to be a Map. Found: class ${workflowArgs['auto'].getClass()}"
  workflowArgs["auto"] = processAuto(defaultWfArgs.auto + workflowArgs["auto"])

  // auto define publish, if so desired
  if (workflowArgs.auto.publish == true && (workflowArgs.directives.publishDir != null ? workflowArgs.directives.publishDir : [:]).isEmpty()) {
    // can't assert at this level thanks to the no_publish profile
    // assert params.containsKey("publishDir") || params.containsKey("publish_dir") : 
    //   "Error in module '${workflowArgs['key']}': if auto.publish is true, params.publish_dir needs to be defined.\n" +
    //   "  Example: params.publish_dir = \"./output/\""
    def publishDir = getPublishDir()
    
    if (publishDir != null) {
      workflowArgs.directives.publishDir = [[ 
        path: publishDir, 
        saveAs: "{ it.startsWith('.') ? null : it }", // don't publish hidden files, by default
        mode: "copy"
      ]]
    }
  }

  // auto define transcript, if so desired
  if (workflowArgs.auto.transcript == true) {
    // can't assert at this level thanks to the no_publish profile
    // assert params.containsKey("transcriptsDir") || params.containsKey("transcripts_dir") || params.containsKey("publishDir") || params.containsKey("publish_dir") : 
    //   "Error in module '${workflowArgs['key']}': if auto.transcript is true, either params.transcripts_dir or params.publish_dir needs to be defined.\n" +
    //   "  Example: params.transcripts_dir = \"./transcripts/\""
    def transcriptsDir = 
      params.containsKey("transcripts_dir") ? params.transcripts_dir : 
      params.containsKey("transcriptsDir") ? params.transcriptsDir : 
      params.containsKey("publish_dir") ? params.publish_dir + "/_transcripts" :
      params.containsKey("publishDir") ? params.publishDir + "/_transcripts" : 
      null
    if (transcriptsDir != null) {
      def timestamp = nextflow.Nextflow.getSession().getWorkflowMetadata().start.format('yyyy-MM-dd_HH-mm-ss')
      def transcriptsPublishDir = [ 
        path: "$transcriptsDir/$timestamp/\${task.process.replaceAll(':', '-')}/\${id}/",
        saveAs: "{ it.startsWith('.') ? it.replaceAll('^.', '') : null }", 
        mode: "copy"
      ]
      def publishDirs = workflowArgs.directives.publishDir != null ? workflowArgs.directives.publishDir : null ? workflowArgs.directives.publishDir : []
      workflowArgs.directives.publishDir = publishDirs + transcriptsPublishDir
    }
  }

  // if this is a stubrun, remove certain directives?
  if (workflow.stubRun) {
    workflowArgs.directives.keySet().removeAll(["publishDir", "cpus", "memory", "label"])
  }

  for (nam in ["map", "mapId", "mapData", "mapPassthrough", "filter", "runIf"]) {
    if (workflowArgs.containsKey(nam) && workflowArgs[nam]) {
      assert workflowArgs[nam] instanceof Closure : "Error in module '$key': Expected process argument '$nam' to be null or a Closure. Found: class ${workflowArgs[nam].getClass()}"
    }
  }

  // TODO: should functions like 'map', 'mapId', 'mapData', 'mapPassthrough' be deprecated as well?
  for (nam in ["map", "mapData", "mapPassthrough", "renameKeys"]) {
    if (workflowArgs.containsKey(nam) && workflowArgs[nam] != null) {
      log.warn "module '$key': workflow argument '$nam' is deprecated and will be removed in Viash 0.9.0. Please use 'fromState' and 'toState' instead."
    }
  }

  // check fromState
  workflowArgs["fromState"] = _processFromState(workflowArgs.get("fromState"), key, meta.config)

  // check toState
  workflowArgs["toState"] = _processToState(workflowArgs.get("toState"), key, meta.config)

  // return output
  return workflowArgs
}

def _processFromState(fromState, key_, config_) {
  assert fromState == null || fromState instanceof Closure || fromState instanceof Map || fromState instanceof List :
    "Error in module '$key_': Expected process argument 'fromState' to be null, a Closure, a Map, or a List. Found: class ${fromState.getClass()}"
  if (fromState == null) {
    return null
  }
  
  // if fromState is a List, convert to map
  if (fromState instanceof List) {
    // check whether fromstate is a list[string]
    assert fromState.every{it instanceof CharSequence} : "Error in module '$key_': fromState is a List, but not all elements are Strings"
    fromState = fromState.collectEntries{[it, it]}
  }

  // if fromState is a map, convert to closure
  if (fromState instanceof Map) {
    // check whether fromstate is a map[string, string]
    assert fromState.values().every{it instanceof CharSequence} : "Error in module '$key_': fromState is a Map, but not all values are Strings"
    assert fromState.keySet().every{it instanceof CharSequence} : "Error in module '$key_': fromState is a Map, but not all keys are Strings"
    def fromStateMap = fromState.clone()
    def requiredInputNames = meta.config.functionality.allArguments.findAll{it.required && it.direction == "Input"}.collect{it.plainName}
    // turn the map into a closure to be used later on
    fromState = { it ->
      def state = it[1]
      assert state instanceof Map : "Error in module '$key_': the state is not a Map"
      def data = fromStateMap.collectMany{newkey, origkey ->
        // check whether newkey corresponds to a required argument
        if (state.containsKey(origkey)) {
          [[newkey, state[origkey]]]
        } else if (!requiredInputNames.contains(origkey)) {
          []
        } else {
          throw new Exception("Error in module '$key_': fromState key '$origkey' not found in current state")
        }
      }.collectEntries()
      data
    }
  }
  
  return fromState
}

def _processToState(toState, key_, config_) {
  if (toState == null) {
    toState = { tup -> tup[1] }
  }

  // toState should be a closure, map[string, string], or list[string]
  assert toState instanceof Closure || toState instanceof Map || toState instanceof List :
    "Error in module '$key_': Expected process argument 'toState' to be a Closure, a Map, or a List. Found: class ${toState.getClass()}"

  // if toState is a List, convert to map
  if (toState instanceof List) {
    // check whether toState is a list[string]
    assert toState.every{it instanceof CharSequence} : "Error in module '$key_': toState is a List, but not all elements are Strings"
    toState = toState.collectEntries{[it, it]}
  }

  // if toState is a map, convert to closure
  if (toState instanceof Map) {
    // check whether toState is a map[string, string]
    assert toState.values().every{it instanceof CharSequence} : "Error in module '$key_': toState is a Map, but not all values are Strings"
    assert toState.keySet().every{it instanceof CharSequence} : "Error in module '$key_': toState is a Map, but not all keys are Strings"
    def toStateMap = toState.clone()
    def requiredOutputNames = config_.functionality.allArguments.findAll{it.required && it.direction == "Output"}.collect{it.plainName}
    // turn the map into a closure to be used later on
    toState = { it ->
      def output = it[1]
      def state = it[2]
      assert output instanceof Map : "Error in module '$key_': the output is not a Map"
      assert state instanceof Map : "Error in module '$key_': the state is not a Map"
      def extraEntries = toStateMap.collectMany{newkey, origkey ->
        // check whether newkey corresponds to a required argument
        if (output.containsKey(origkey)) {
          [[newkey, output[origkey]]]
        } else if (!requiredOutputNames.contains(origkey)) {
          []
        } else {
          throw new Exception("Error in module '$key_': toState key '$origkey' not found in current output")
        }
      }.collectEntries()
      state + extraEntries
    }
  }

  return toState
}

// helper file: 'src/main/resources/io/viash/platforms/nextflow/workflowFactory/workflowFactory.nf'
def _debug(workflowArgs, debugKey) {
  if (workflowArgs.debug) {
    view { "process '${workflowArgs.key}' $debugKey tuple: $it"  }
  } else {
    map { it }
  }
}

// depends on: innerWorkflowFactory
def workflowFactory(Map args, Map defaultWfArgs, Map meta) {
  def workflowArgs = processWorkflowArgs(args, defaultWfArgs, meta)
  def key_ = workflowArgs["key"]
  
  workflow workflowInstance {
    take: input_

    main:
    chModified = input_
      | checkUniqueIds([:])
      | _debug(workflowArgs, "input")
      | map { tuple ->
        tuple = tuple.clone()
        
        if (workflowArgs.map) {
          tuple = workflowArgs.map(tuple)
        }
        if (workflowArgs.mapId) {
          tuple[0] = workflowArgs.mapId(tuple[0])
        }
        if (workflowArgs.mapData) {
          tuple[1] = workflowArgs.mapData(tuple[1])
        }
        if (workflowArgs.mapPassthrough) {
          tuple = tuple.take(2) + workflowArgs.mapPassthrough(tuple.drop(2))
        }

        // check tuple
        assert tuple instanceof List : 
          "Error in module '${key_}': element in channel should be a tuple [id, data, ...otherargs...]\n" +
          "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
          "  Expected class: List. Found: tuple.getClass() is ${tuple.getClass()}"
        assert tuple.size() >= 2 : 
          "Error in module '${key_}': expected length of tuple in input channel to be two or greater.\n" +
          "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
          "  Found: tuple.size() == ${tuple.size()}"
        
        // check id field
        assert tuple[0] instanceof CharSequence : 
          "Error in module '${key_}': first element of tuple in channel should be a String\n" +
          "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
          "  Found: ${tuple[0]}"
        
        // match file to input file
        if (workflowArgs.auto.simplifyInput && (tuple[1] instanceof Path || tuple[1] instanceof List)) {
          def inputFiles = meta.config.functionality.allArguments
            .findAll { it.type == "file" && it.direction == "input" }
          
          assert inputFiles.size() == 1 : 
              "Error in module '${key_}' id '${tuple[0]}'.\n" +
              "  Anonymous file inputs are only allowed when the process has exactly one file input.\n" +
              "  Expected: inputFiles.size() == 1. Found: inputFiles.size() is ${inputFiles.size()}"

          tuple[1] = [[ inputFiles[0].plainName, tuple[1] ]].collectEntries()
        }

        // check data field
        assert tuple[1] instanceof Map : 
          "Error in module '${key_}' id '${tuple[0]}': second element of tuple in channel should be a Map\n" +
          "  Example: [\"id\", [input: file('foo.txt'), arg: 10]].\n" +
          "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"

        // rename keys of data field in tuple
        if (workflowArgs.renameKeys) {
          assert workflowArgs.renameKeys instanceof Map : 
              "Error renaming data keys in module '${key_}' id '${tuple[0]}'.\n" +
              "  Example: renameKeys: ['new_key': 'old_key'].\n" +
              "  Expected class: Map. Found: renameKeys.getClass() is ${workflowArgs.renameKeys.getClass()}"
          assert tuple[1] instanceof Map : 
              "Error renaming data keys in module '${key_}' id '${tuple[0]}'.\n" +
              "  Expected class: Map. Found: tuple[1].getClass() is ${tuple[1].getClass()}"

          // TODO: allow renameKeys to be a function?
          workflowArgs.renameKeys.each { newKey, oldKey ->
            assert newKey instanceof CharSequence : 
              "Error renaming data keys in module '${key_}' id '${tuple[0]}'.\n" +
              "  Example: renameKeys: ['new_key': 'old_key'].\n" +
              "  Expected class of newKey: String. Found: newKey.getClass() is ${newKey.getClass()}"
            assert oldKey instanceof CharSequence : 
              "Error renaming data keys in module '${key_}' id '${tuple[0]}'.\n" +
              "  Example: renameKeys: ['new_key': 'old_key'].\n" +
              "  Expected class of oldKey: String. Found: oldKey.getClass() is ${oldKey.getClass()}"
            assert tuple[1].containsKey(oldKey) : 
              "Error renaming data keys in module '${key}' id '${tuple[0]}'.\n" +
              "  Key '$oldKey' is missing in the data map. tuple[1].keySet() is '${tuple[1].keySet()}'"
            tuple[1].put(newKey, tuple[1][oldKey])
          }
          tuple[1].keySet().removeAll(workflowArgs.renameKeys.collect{ newKey, oldKey -> oldKey })
        }
        tuple
      }

    if (workflowArgs.filter) {
      chModifiedFiltered = chModified
        | filter{workflowArgs.filter(it)}
    } else {
      chModifiedFiltered = chModified
    }

    if (workflowArgs.runIf) {
      runIfBranch = chModifiedFiltered.branch{ tup ->
        run: workflowArgs.runIf(tup[0], tup[1])
        passthrough: true
      }
      chRun = runIfBranch.run
      chPassthrough = runIfBranch.passthrough
    } else {
      chRun = chModifiedFiltered
      chPassthrough = Channel.empty()
    }

    if (workflowArgs.fromState) {
      chArgs = chRun
        | map{
          def new_data = workflowArgs.fromState(it.take(2))
          [it[0], new_data]
        }
    } else {
      chArgs = chRun
    }

    // fill in defaults
    chArgsWithDefaults = chArgs
      | map { tuple ->
        def id_ = tuple[0]
        def data_ = tuple[1]

        // TODO: could move fromState to here

        // fetch default params from functionality
        def defaultArgs = meta.config.functionality.allArguments
          .findAll { it.containsKey("default") }
          .collectEntries { [ it.plainName, it.default ] }

        // fetch overrides in params
        def paramArgs = meta.config.functionality.allArguments
          .findAll { par ->
            def argKey = key_ + "__" + par.plainName
            params.containsKey(argKey)
          }
          .collectEntries { [ it.plainName, params[key_ + "__" + it.plainName] ] }
        
        // fetch overrides in data
        def dataArgs = meta.config.functionality.allArguments
          .findAll { data_.containsKey(it.plainName) }
          .collectEntries { [ it.plainName, data_[it.plainName] ] }
        
        // combine params
        def combinedArgs = defaultArgs + paramArgs + workflowArgs.args + dataArgs

        // remove arguments with explicit null values
        combinedArgs
          .removeAll{_, val -> val == null || val == "viash_no_value" || val == "force_null"}

        combinedArgs = _processInputValues(combinedArgs, meta.config, id_, key_)

        [id_, combinedArgs] + tuple.drop(2)
      }

    // TODO: move some of the _meta.join_id wrangling to the safeJoin() function.
    chInitialOutput = chArgsWithDefaults
      | _debug(workflowArgs, "processed")
      // run workflow
      | innerWorkflowFactory(workflowArgs)
      // check output tuple
      | map { id_, output_ ->

        // see if output map contains metadata
        def meta_ =
          output_ instanceof Map && output_.containsKey("_meta") ? 
          output_["_meta"] :
          [:]
        def join_id = meta_.join_id ?: id_
        
        // remove metadata
        output_ = output_.findAll{k, v -> k != "_meta"}

        // check value types
        output_ = _processOutputValues(output_, meta.config, id_, key_)

        // simplify output if need be
        if (workflowArgs.auto.simplifyOutput && output_.size() == 1) {
          output_ = output_.values()[0]
        }

        [join_id, id_, output_]
      }
      // | view{"chInitialOutput: ${it.take(3)}"}

    // join the output [prev_id, new_id, output] with the previous state [prev_id, state, ...]
    chNewState = safeJoin(chInitialOutput, chModifiedFiltered, key_)
      // input tuple format: [join_id, id, output, prev_state, ...]
      // output tuple format: [join_id, id, new_state, ...]
      | map{ tup ->
        def new_state = workflowArgs.toState(tup.drop(1).take(3))
        tup.take(2) + [new_state] + tup.drop(4)
      }

    if (workflowArgs.auto.publish == "state") {
      chPublish = chNewState
        // input tuple format: [join_id, id, new_state, ...]
        // output tuple format: [join_id, id, new_state]
        | map{ tup ->
          tup.take(3)
        }

      safeJoin(chPublish, chArgsWithDefaults, key_)
        // input tuple format: [join_id, id, new_state, orig_state, ...]
        // output tuple format: [id, new_state, orig_state]
        | map { tup ->
          tup.drop(1).take(3)
      }
        | publishStatesByConfig(key: key_, config: meta.config)
    }

    // remove join_id and meta
    chReturn = chNewState
      | map { tup ->
        // input tuple format: [join_id, id, new_state, ...]
        // output tuple format: [id, new_state, ...]
        tup.drop(1)
      }
      | _debug(workflowArgs, "output")
      | concat(chPassthrough)

    emit: chReturn
  }

  def wf = workflowInstance.cloneWithName(key_)

  // add factory function
  wf.metaClass.run = { runArgs ->
    workflowFactory(runArgs, workflowArgs, meta)
  }
  // add config to module for later introspection
  wf.metaClass.config = meta.config

  return wf
}

nextflow.enable.dsl=2

// START COMPONENT-SPECIFIC CODE

// create meta object
meta = [
  "resources_dir": moduleDir.normalize(),
  "config": processConfig(readJsonBlob('''{
  "functionality" : {
    "name" : "rnaseq",
    "namespace" : "workflows",
    "version" : "dev",
    "arguments" : [
      {
        "type" : "string",
        "name" : "--id",
        "description" : "ID of the sample.",
        "example" : [
          "foo"
        ],
        "required" : true,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--fastq_1",
        "description" : "Path to the sample (or read 1 of paired end sample).",
        "must_exist" : true,
        "create_parent" : true,
        "required" : true,
        "direction" : "input",
        "multiple" : true,
        "multiple_sep" : ";",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--fastq_2",
        "description" : "Path to read 2 of the sample.",
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : true,
        "multiple_sep" : ";",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--strandedness",
        "description" : "Sample strand-specificity. Must be one of unstranded, forward, reverse or auto",
        "default" : [
          "auto"
        ],
        "required" : false,
        "choices" : [
          "unstranded",
          "forward",
          "reverse",
          "auto"
        ],
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--fasta",
        "description" : "Path to FASTA genome file.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : true,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--gtf",
        "description" : "Path to GTF annotation file. This parameter is *mandatory* if --genome is not specified.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--gff",
        "description" : "Path to GFF3 annotation file. Required if \\"--gtf\\" is not specified.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--additional_fasta",
        "description" : "FASTA file to concatenate to genome FASTA file e.g. containing spike-in sequences.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--transcript_fasta",
        "description" : "Path to FASTA transcriptome file.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--gene_bed",
        "description" : "Path to BED file containing gene intervals. This will be created from the GTF file if not specified.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--splicesites",
        "description" : "Splice sites file required for HISAT2.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--star_index",
        "description" : "Path to directory or tar.gz archive for pre-built STAR index.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--rsem_index",
        "description" : "Path to directory or tar.gz archive for pre-built RSEM index.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--salmon_index",
        "description" : "Path to directory or tar.gz archive for pre-built Salmon index.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--gencode",
        "description" : "Specify if the GTF annotation is in GENCODE format.",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--gtf_extra_attributes",
        "description" : "Additional gene identifiers from the input GTF file when running Salmon. More than one value can be specified separated by comma.",
        "default" : [
          "gene_name"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--gtf_group_features",
        "description" : "Define the attribute type used to group features in the GTF file when running Salmon.",
        "default" : [
          "gene_id"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--featurecounts_group_type",
        "description" : "The attribute type used to group feature types in the GTF file when generating the biotype plot with featureCounts.",
        "default" : [
          "gene_biotype"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--featurecounts_feature_type",
        "description" : "By default, the pipeline assigns reads based on the 'exon' attribute within the GTF file.",
        "default" : [
          "exon"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--save_reference",
        "description" : "If generated by the pipeline, save the STAR index in the results directory.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--skip_fastqc",
        "description" : "Skip FatQC step.",
        "default" : [
          false
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--with_umi",
        "description" : "Enable UMI-based read deduplication.",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--skip_umi_extract",
        "description" : "Skip umi_tools extract step.",
        "default" : [
          false
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--umitools_extract_method",
        "description" : "UMI pattern to use.",
        "default" : [
          "string"
        ],
        "required" : false,
        "choices" : [
          "string",
          "regex"
        ],
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--umitools_bc_pattern",
        "description" : "The UMI barcode pattern to use e.g. 'NNNNNN' indicates that the first 6 nucleotides of the read are from the UMI.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--umitools_bc_pattern2",
        "description" : "The UMI barcode pattern to use if the UMI is located in read 2.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "integer",
        "name" : "--umi_discard_read",
        "description" : "After UMI barcode extraction discard either R1 or R2 by setting this parameter to 1 or 2, respectively.",
        "default" : [
          0
        ],
        "required" : false,
        "choices" : [
          0,
          1,
          2
        ],
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--umitools_umi_separator",
        "description" : "The character that separates the UMI in the read name. Most likely a colon if you skipped the extraction with UMI-tools and used other software.",
        "default" : [
          "_"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--umitools_grouping_method",
        "description" : "Method to use to determine read groups by subsuming those with similar UMIs. All methods start by identifying the reads with the same mapping position, but treat similar yet nonidentical UMIs differently.",
        "default" : [
          "directional"
        ],
        "required" : false,
        "choices" : [
          "unique",
          "percentile",
          "cluster",
          "adjacency",
          "directional"
        ],
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--save_umi_intermeds",
        "description" : "If this option is specified, intermediate FastQ and BAM files produced by UMI-tools are also saved in the results directory.",
        "default" : [
          false
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--umi_dedup_stats",
        "description" : "Generate output stats when running \\"umi_tools dedup\\".",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--trimmer",
        "description" : "Specify the trimming tool to use.",
        "default" : [
          "trimgalore"
        ],
        "required" : false,
        "choices" : [
          "trimgalore",
          "fastp"
        ],
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_trimgalore_args",
        "description" : "Extra arguments to pass to Trim Galore! command in addition to defaults defined by the pipeline.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_fastp_args",
        "description" : "Extra arguments to pass to fastp command in addition to defaults defined by the pipeline.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "integer",
        "name" : "--min_trimmed_reads",
        "description" : "Minimum number of trimmed reads below which samples are removed from further processing. Some downstream steps in the pipeline will fail if this threshold is too low.",
        "default" : [
          10000
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--skip_trimming",
        "description" : "Skip the adapter trimming step.",
        "default" : [
          false
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--save_trimmed",
        "description" : "Save the trimmed FastQ files in the results directory.",
        "default" : [
          false
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--bbsplit_fasta_list",
        "description" : "Path to comma-separated file containing a list of reference genomes to filter reads against with BBSplit. To use BBSplit, \\"--skip_bbsplit\\" must be explicitly set to \\"false\\". The file should contain 2 (comma separated) columns - short name and full path to reference genome(s)",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--bbsplit_index",
        "description" : "Path to directory or tar.gz archive for pre-built BBSplit index.",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--save_bbsplit_reads",
        "description" : "If this option is specified, FastQ files split by reference will be saved in the results directory.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--skip_bbsplit",
        "description" : "Skip BBSplit for removal of non-reference genome reads.",
        "default" : [
          true
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--remove_ribo_rna",
        "description" : "Enable the removal of reads derived from ribosomal RNA using SortMeRNA.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--ribo_database_manifest",
        "description" : "Text file containing paths to fasta files (one per line) that will be used to create the database for SortMeRNA.",
        "default" : [
          "assets/rrna-db-defaults.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--save_non_ribo_reads",
        "description" : "If this option is specified, intermediate FastQ files containing non-rRNA reads will be saved in the results directory.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "integer",
        "name" : "--min_mapped_reads",
        "description" : "Minimum percentage of uniquely mapped reads below which samples are removed from further processing.",
        "default" : [
          5
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--bam_csi_index",
        "description" : "Create a CSI index for BAM files instead of the traditional BAI index. This will be required for genomes with larger chromosome sizes.",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_star_align_args",
        "description" : "Extra arguments to pass to STAR alignment command in addition to defaults defined by the pipeline.",
        "default" : [
          "--readFilesCommand gunzip -c --quantMode TranscriptomeSAM --twopassMode Basic --outSAMtype BAM Unsorted --runRNGseed 0 --outFilterMultimapNmax 20 --alignSJDBoverhangMin 1 --outSAMattributes NH HI AS NM MD --quantTranscriptomeBan Singleend --outSAMstrandField intronMotif"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--star_ignore_sjdbgtf",
        "description" : "When using pre-built STAR indices, do not re-extract and use splice junctions from the GTF file.",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--seq_platform",
        "description" : "Sequencing platform.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--seq_center",
        "description" : "Sequencing center.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--salmon_quant_libtype",
        "description" : "Override Salmon library type inferred based on strandedness defined in meta object.",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_salmon_quant_args",
        "description" : "Extra arguments to pass to salmon quant command in addition to defaults defined by the pipeline.",
        "default" : [
          "-v"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--stringtie_ignore_gtf",
        "description" : "Perform reference-guided de novo assembly of transcripts using StringTie, i.e. don't restrict to those in GTF file.",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_stringtie_args",
        "description" : "Extra arguments to pass to stringtie command in addition to defaults defined by the pipeline.",
        "default" : [
          "-v"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--skip_qc",
        "description" : "Skip QC steps of the workflow.",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--skip_markdupkicates",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--skip_stringtie",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--skip_biotype_qc",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--skip_bigwig",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--skip_preseq",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "boolean_true",
        "name" : "--skip_deseq2_qc",
        "direction" : "input",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_fq_subsample_args",
        "description" : "Extra arguments to pass to fq subsample command in addition to defaults defined by the pipeline.",
        "default" : [
          "--record-count 1000000 --seed 1"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_picard_args",
        "description" : "Extra arguments to pass to picard MarkDuplicates command in addition to defaults defined by the pipeline.",
        "default" : [
          "--ASSUME_SORTED true --REMOVE_DUPLICATES false --VALIDATION_STRINGENCY LENIENT --TMP_DIR tmp"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_bedtools_args",
        "description" : "Extra arguments to pass to bedtools genomecov command in addition to defaults defined by the pipeline.",
        "default" : [
          "-split -du"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_featurecounts_args",
        "description" : "Extra arguments to pass to featureCounts command in addition to defaults defined by the pipeline",
        "default" : [
          "-B -C"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_preseq_args",
        "description" : "Extra arguments to pass to preseq lc_extrap command in addition to defaults defined by the pipeline",
        "default" : [
          "-verbose -seed 1 -seg_len 100000000"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--pca_header_multiqc",
        "default" : [
          "assets/multiqc/deseq2_pca_header.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--clustering_header_multiqc",
        "default" : [
          "assets/multiqc/deseq2_clustering_header.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "boolean",
        "name" : "--deseq2_vst",
        "description" : "Use vst transformation instead of rlog with DESeq2",
        "default" : [
          true
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_deseq2_args",
        "default" : [
          "--id_col 1 --sample_suffix '' --outprefix deseq2 --count_col 3"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--extra_deseq2_args2",
        "default" : [
          "star_salmon"
        ],
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--rseqc_modules",
        "description" : "Specify the RSeQC modules to run_wf",
        "default" : [
          "bam_stat,inner_distance,infer_experiment,junction_annotation,junction_saturation,read_distribution,read_duplication"
        ],
        "required" : false,
        "choices" : [
          "bam_stat",
          "inner_distance",
          "infer_experiment",
          "junction_annotation",
          "junction_saturation",
          "read_distribution",
          "read_duplication",
          "tin"
        ],
        "direction" : "input",
        "multiple" : true,
        "multiple_sep" : ",",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--multiqc_custom_config",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "string",
        "name" : "--multiqc_title",
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--multiqc_logo",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--multiqc_methods_description",
        "default" : [
          "assets/methods_description_template.yml"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "input",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--output_fasta",
        "default" : [
          "genome/reference_genome.fasta"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--output_gtf",
        "default" : [
          "genome/gene_annotation.gtf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--output_transcript_fasta",
        "default" : [
          "genome/transcriptome.fasta"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--output_gene_bed",
        "default" : [
          "genome/gene_annotation.bed"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--output_star_index",
        "description" : "Path to STAR index.",
        "default" : [
          "genome/index/STAR"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--output_salmon_index",
        "description" : "Path to Salmon index.",
        "default" : [
          "genome/index/salmon"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--output_bbsplit_index",
        "description" : "Path to BBSplit index.",
        "default" : [
          "genome/index/bbsplit"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--output_fastq_1",
        "description" : "Path to output directory",
        "default" : [
          "fastq/$id.read1.fastq.gz"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--output_fastq_2",
        "description" : "Path to output directory",
        "default" : [
          "fastq/$id.read2.fastq.gz"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--fastqc_html_1",
        "description" : "FastQC HTML report for read 1.",
        "default" : [
          "fastqc/$id.read1.fastqc.html"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--fastqc_html_2",
        "description" : "FastQC HTML report for read 2.",
        "default" : [
          "fastqc/$id.read2.fastqc.html"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--fastqc_zip_1",
        "description" : "FastQC report archive for read 1.",
        "default" : [
          "fastqc/$id.read1.fastqc.zip"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--fastqc_zip_2",
        "description" : "FastQC report archive for read 2.",
        "default" : [
          "fastqc/$id.read2.fastqc.zip"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--trim_log_1",
        "default" : [
          "trimgalore/$id.read1.trimming_report.txt"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--trim_log_2",
        "default" : [
          "trimgalore/$id.read2.trimming_report.txt"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--trim_html_1",
        "default" : [
          "trimgalore/$id.read1.trimmed_fastqc.html"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--trim_html_2",
        "default" : [
          "trimgalore/$id.read2.trimmed_fastqc.html"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--trim_zip_1",
        "default" : [
          "trimgalore/$id.read1.trimmed_fastqc.zip"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--trim_zip_2",
        "default" : [
          "trimgalore/$id.read2.trimmed_fastqc.zip"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--sortmerna_log",
        "description" : "Sortmerna log file.",
        "default" : [
          "sortmerna/$id.log"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--star_alignment",
        "default" : [
          "STAR_alignment/$id.STAR"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--genome_bam_sorted",
        "default" : [
          "$id.genome.bam"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--genome_bam_index",
        "default" : [
          "$id.genome.bam.bai"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--genome_bam_stats",
        "default" : [
          "samtools_stats/$id.genome.stats"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--genome_bam_flagstat",
        "default" : [
          "samtools_stats/$id.genome.flagstat"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--genome_bam_idxstats",
        "default" : [
          "samtools_stats/$id.genome.idxstats"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--transcriptome_bam_sorted",
        "default" : [
          "$id.transcriptome.bam"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--transcriptome_bam_index",
        "default" : [
          "$id.transcriptome.bam.bai"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--transcriptome_bam_stats",
        "default" : [
          "samtools_stats/$id.transcriptome.stats"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--transcriptome_bam_flagstat",
        "default" : [
          "samtools_stats/$id.transcriptome.flagstat"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--transcriptome_bam_idxstats",
        "default" : [
          "samtools_stats/$id.transcriptome.idxstats"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--salmon_quant_results",
        "default" : [
          "align_quantified/$id.salmon_quant"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--markduplicates_metrics",
        "default" : [
          "picard_metrics/$id.markdup.sorted.MarkDuplicates.metrics.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--stringtie_transcript_gtf",
        "default" : [
          "stringtie/$id.transcripts.gtf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--stringtie_coverage_gtf",
        "default" : [
          "stringtie/$id.coverage.gtf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--stringtie_abundance",
        "default" : [
          "stringtie/$id.gene_abundance.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--stringtie_ballgown",
        "default" : [
          "stringtie/$id.ballgown"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--featurecounts",
        "default" : [
          "featurecounts/$id.featureCounts.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--featurecounts_summary",
        "default" : [
          "featurecounts/$id.featureCounts.txt.summary"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--bedgraph_forward",
        "default" : [
          "bedgraph/$id.forward.bedgraph"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--bedgraph_reverse",
        "default" : [
          "bedgraph/$id.reverse.bedgraph"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--bigwig_forward",
        "default" : [
          "bigwig/$id.forward.bigwig"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--bigwig_reverse",
        "default" : [
          "bigwig/$id.reverse.bigwig"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--preseq_output",
        "default" : [
          "preseq/$id.lc_extrap.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--bamstat_output",
        "description" : "Path to output file (txt) of mapping quality statistics",
        "default" : [
          "RSeQC/bamstat/$id.mapping_quality.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--strandedness_output",
        "description" : "Path to output report (txt) of inferred strandedness",
        "default" : [
          "RSeQC/inferexperiment/$id.strandedness.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--inner_dist_output_stats",
        "description" : "output file (txt) with summary statistics of inner distances of paired reads",
        "default" : [
          "RSeQC/innerdistance/$id.inner_distance.stats"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--inner_dist_output_dist",
        "description" : "output file (txt) with inner distances of all paired reads",
        "default" : [
          "RSeQC/innerdistance/$id.inner_distance.txt"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--inner_dist_output_freq",
        "description" : "output file (txt) with frequencies of inner distances of all paired reads",
        "default" : [
          "RSeQC/innerdistance/$id.inner_distance_freq.txt"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--inner_dist_output_plot",
        "description" : "output file (pdf) with histogram plot of of inner distances of all paired reads",
        "default" : [
          "RSeQC/innerdistance/$id.inner_distance_plot.pdf"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--inner_dist_output_plot_r",
        "description" : "output file (R) with script of histogram plot of of inner distances of all paired reads",
        "default" : [
          "RSeQC/innerdistance/$id.inner_distance_plot.r"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--junction_annotation_output_log",
        "description" : "output log of junction annotation script",
        "default" : [
          "RSeQC/junctionannotation/$id.junction_annotation.log"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--junction_annotation_output_plot_r",
        "description" : "R script to generate splice_junction and splice_events plot",
        "default" : [
          "RSeQC/junctionannotation/$id.junction_annotation_plot.r"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--junction_annotation_output_junction_bed",
        "description" : "junction annotation file (bed format)",
        "default" : [
          "RSeQC/junctionannotation/$id.junction_annotation.bed"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--junction_annotation_output_junction_interact",
        "description" : "interact file (bed format) of junctions. Can be uploaded to UCSC genome browser or converted to bigInteract (using bedToBigBed program) for visualization.",
        "default" : [
          "RSeQC/junctionannotation/$id.junction_annotation.Interact.bed"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--junction_annotation_output_junction_sheet",
        "description" : "junction annotation file (xls format)",
        "default" : [
          "RSeQC/junctionannotation/$id.junction_annotation.xls"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--junction_annotation_output_splice_events_plot",
        "description" : "plot of splice events (pdf)",
        "default" : [
          "RSeQC/junctionannotation/$id.splice_events.pdf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--junction_annotation_output_splice_junctions_plot",
        "description" : "plot of junctions (pdf)",
        "default" : [
          "RSeQC/junctionannotation/$id.splice_junctions_plot.pdf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--junction_saturation_output_plot_r",
        "description" : "r script to generate junction_saturation_plot plot",
        "default" : [
          "RSeQC/junctionsaturation/$id.junction_saturation_plot.r"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--junction_saturation_output_plot",
        "description" : "plot of junction saturation (pdf",
        "default" : [
          "RSeQC/junctionsaturation/$id.junction_saturation_plot.pdf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--read_distribution_output",
        "description" : "output file (txt) of read distribution analysis.",
        "default" : [
          "RSeQC/readdistribution/$id.read_distribution.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--read_duplication_output_duplication_rate_plot_r",
        "description" : "R script for generating duplication rate plot",
        "default" : [
          "RSeQC/readduplication/$id.duplication_rate_plot.r"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--read_duplication_output_duplication_rate_plot",
        "description" : "duplication rate plot (pdf)",
        "default" : [
          "RSeQC/readduplication/$id.duplication_rate_plot.pdf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--read_duplication_output_duplication_rate_mapping",
        "description" : "Summary of mapping-based read duplication",
        "default" : [
          "RSeQC/readduplication/$id.duplication_rate_mapping.xls"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--read_duplication_output_duplication_rate_sequence",
        "description" : "Summary of sequencing-based read duplication",
        "default" : [
          "RSeQC/readduplication/$id.duplication_rate_sequencing.xls"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--tin_output_summary",
        "description" : "summary statistics (txt) of calculated TIN metrics",
        "default" : [
          "RSeQC/tin/$id.tin_summary.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--tin_output_metrics",
        "description" : "file with TIN metrics (xls)",
        "default" : [
          "RSeQC/tin/$id.tin.xls"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--dupradar_output_dupmatrix",
        "description" : "path to output file (txt) of duplicate tag counts",
        "default" : [
          "dupradar/$id.dup_matrix.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--dupradar_output_dup_intercept_mqc",
        "description" : "path to output file (txt) of multiqc intercept value DupRadar",
        "default" : [
          "dupradar/$id.dup_intercept_mqc.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--dupradar_output_duprate_exp_boxplot",
        "description" : "path to output file (pdf) of distribution of expression box plot",
        "default" : [
          "dupradar/$id.duprate_exp_boxplot.pdf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--dupradar_output_duprate_exp_densplot",
        "description" : "path to output file (pdf) of 2D density scatter plot of duplicate tag counts",
        "default" : [
          "dupradar/$id.duprate_exp_densityplot.pdf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--dupradar_output_duprate_exp_denscurve_mqc",
        "description" : "path to output file (pdf) of density curve of gene duplication multiqc",
        "default" : [
          "dupradar/$id.duprate_exp_density_curve_mqc.pdf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--dupradar_output_expression_histogram",
        "description" : "path to output file (pdf) of distribution of RPK values per gene histogram",
        "default" : [
          "dupradar/$id.expression_hist.pdf"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--dupradar_output_intercept_slope",
        "default" : [
          "dupradar/$id.intercept_slope.txt"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--qualimap_output_pdf",
        "default" : [
          "qualimap/$id.qualimap_output.pdf"
        ],
        "must_exist" : false,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--qualimap_output_dir",
        "default" : [
          "qualimap/$id.qualimap_output"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--deseq2_output",
        "default" : [
          "deseq2"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--pca_multiqc",
        "default" : [
          "deseq2.pca.vals_mqc.tsv"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--dists_multiqc",
        "default" : [
          "deseq2.sample.dists_mqc.tsv"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--multiqc_report",
        "default" : [
          "multiqc/multiqc_report.html"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--multiqc_data",
        "default" : [
          "multiqc/multiqc_data"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--multiqc_plots",
        "default" : [
          "multiqc/multiqc_plots"
        ],
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      },
      {
        "type" : "file",
        "name" : "--multiqc_versions",
        "must_exist" : true,
        "create_parent" : true,
        "required" : false,
        "direction" : "output",
        "multiple" : false,
        "multiple_sep" : ":",
        "dest" : "par"
      }
    ],
    "resources" : [
      {
        "type" : "nextflow_script",
        "path" : "main.nf",
        "is_executable" : true,
        "parent" : "file:/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/rnaseq/",
        "entrypoint" : "run_wf"
      }
    ],
    "description" : "A viash workflow for the nf-core/rnaseq pipeline.\n",
    "status" : "enabled",
    "requirements" : {
      "commands" : [
        "ps"
      ]
    },
    "dependencies" : [
      {
        "name" : "workflows/prepare_genome",
        "repository" : {
          "type" : "local",
          "name" : "",
          "localPath" : ""
        },
        "foundConfigPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/prepare_genome/config.vsh.yaml",
        "configInfo" : {
          "functionalityName" : "prepare_genome",
          "git_tag" : "",
          "git_remote" : "https://github.com/data-intuitive/rnaseq.vsh.git",
          "viash_version" : "0.8.0-RC5",
          "config" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/prepare_genome/config.vsh.yaml",
          "functionalityNamespace" : "workflows",
          "output" : "",
          "platform" : "",
          "git_commit" : "efa0ffd2fbdce950e208caa1570584b258689789",
          "executable" : "/nextflow/workflows/prepare_genome/main.nf"
        },
        "writtenPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/target/nextflow/workflows/prepare_genome"
      },
      {
        "name" : "cat_fastq",
        "repository" : {
          "type" : "local",
          "name" : "",
          "localPath" : ""
        },
        "foundConfigPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/cat_fastq/config.vsh.yaml",
        "configInfo" : {
          "functionalityName" : "cat_fastq",
          "git_tag" : "",
          "git_remote" : "https://github.com/data-intuitive/rnaseq.vsh.git",
          "viash_version" : "0.8.0-RC5",
          "config" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/cat_fastq/config.vsh.yaml",
          "functionalityNamespace" : "",
          "output" : "",
          "platform" : "",
          "git_commit" : "efa0ffd2fbdce950e208caa1570584b258689789",
          "executable" : "/nextflow/cat_fastq/main.nf"
        },
        "writtenPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/target/nextflow/cat_fastq"
      },
      {
        "name" : "workflows/pre_processing",
        "repository" : {
          "type" : "local",
          "name" : "",
          "localPath" : ""
        },
        "foundConfigPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/pre_processing/config.vsh.yaml",
        "configInfo" : {
          "functionalityName" : "pre_processing",
          "git_tag" : "",
          "git_remote" : "https://github.com/data-intuitive/rnaseq.vsh.git",
          "viash_version" : "0.8.0-RC5",
          "config" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/pre_processing/config.vsh.yaml",
          "functionalityNamespace" : "workflows",
          "output" : "",
          "platform" : "",
          "git_commit" : "efa0ffd2fbdce950e208caa1570584b258689789",
          "executable" : "/nextflow/workflows/pre_processing/main.nf"
        },
        "writtenPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/target/nextflow/workflows/pre_processing"
      },
      {
        "name" : "workflows/genome_alignment_and_quant",
        "repository" : {
          "type" : "local",
          "name" : "",
          "localPath" : ""
        },
        "foundConfigPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/genome_alignment_and_quant/config.vsh.yaml",
        "configInfo" : {
          "functionalityName" : "genome_alignment_and_quant",
          "git_tag" : "",
          "git_remote" : "https://github.com/data-intuitive/rnaseq.vsh.git",
          "viash_version" : "0.8.0-RC5",
          "config" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/genome_alignment_and_quant/config.vsh.yaml",
          "functionalityNamespace" : "workflows",
          "output" : "",
          "platform" : "",
          "git_commit" : "efa0ffd2fbdce950e208caa1570584b258689789",
          "executable" : "/nextflow/workflows/genome_alignment_and_quant/main.nf"
        },
        "writtenPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/target/nextflow/work''' + '''flows/genome_alignment_and_quant"
      },
      {
        "name" : "workflows/post_processing",
        "repository" : {
          "type" : "local",
          "name" : "",
          "localPath" : ""
        },
        "foundConfigPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/post_processing/config.vsh.yaml",
        "configInfo" : {
          "functionalityName" : "post_processing",
          "git_tag" : "",
          "git_remote" : "https://github.com/data-intuitive/rnaseq.vsh.git",
          "viash_version" : "0.8.0-RC5",
          "config" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/post_processing/config.vsh.yaml",
          "functionalityNamespace" : "workflows",
          "output" : "",
          "platform" : "",
          "git_commit" : "efa0ffd2fbdce950e208caa1570584b258689789",
          "executable" : "/nextflow/workflows/post_processing/main.nf"
        },
        "writtenPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/target/nextflow/workflows/post_processing"
      },
      {
        "name" : "workflows/quality_control",
        "repository" : {
          "type" : "local",
          "name" : "",
          "localPath" : ""
        },
        "foundConfigPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/quality_control/config.vsh.yaml",
        "configInfo" : {
          "functionalityName" : "quality_control",
          "git_tag" : "",
          "git_remote" : "https://github.com/data-intuitive/rnaseq.vsh.git",
          "viash_version" : "0.8.0-RC5",
          "config" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/quality_control/config.vsh.yaml",
          "functionalityNamespace" : "workflows",
          "output" : "",
          "platform" : "",
          "git_commit" : "efa0ffd2fbdce950e208caa1570584b258689789",
          "executable" : "/nextflow/workflows/quality_control/main.nf"
        },
        "writtenPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/target/nextflow/workflows/quality_control"
      },
      {
        "name" : "workflows/salmon_quant_merge_counts",
        "repository" : {
          "type" : "local",
          "name" : "",
          "localPath" : ""
        },
        "foundConfigPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/salmon_quant_merge_counts/config.vsh.yaml",
        "configInfo" : {
          "functionalityName" : "salmon_quant_merge_counts",
          "git_tag" : "",
          "git_remote" : "https://github.com/data-intuitive/rnaseq.vsh.git",
          "viash_version" : "0.8.0-RC5",
          "config" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/salmon_quant_merge_counts/config.vsh.yaml",
          "functionalityNamespace" : "workflows",
          "output" : "",
          "platform" : "",
          "git_commit" : "efa0ffd2fbdce950e208caa1570584b258689789",
          "executable" : "/nextflow/workflows/salmon_quant_merge_counts/main.nf"
        },
        "writtenPath" : "/home/nirmayi/data_intuitive/rnaseq.vsh/target/nextflow/workflows/salmon_quant_merge_counts"
      }
    ],
    "set_wd_to_resources_dir" : false
  },
  "platforms" : [
    {
      "type" : "nextflow",
      "id" : "nextflow",
      "directives" : {
        "tag" : "$id"
      },
      "auto" : {
        "simplifyInput" : true,
        "simplifyOutput" : false,
        "transcript" : false,
        "publish" : false
      },
      "config" : {
        "labels" : {
          "mem1gb" : "memory = 1.GB",
          "mem2gb" : "memory = 2.GB",
          "mem4gb" : "memory = 4.GB",
          "mem8gb" : "memory = 8.GB",
          "mem16gb" : "memory = 16.GB",
          "mem32gb" : "memory = 32.GB",
          "mem64gb" : "memory = 64.GB",
          "mem128gb" : "memory = 128.GB",
          "mem256gb" : "memory = 256.GB",
          "mem512gb" : "memory = 512.GB",
          "mem1tb" : "memory = 1.TB",
          "mem2tb" : "memory = 2.TB",
          "mem4tb" : "memory = 4.TB",
          "mem8tb" : "memory = 8.TB",
          "mem16tb" : "memory = 16.TB",
          "mem32tb" : "memory = 32.TB",
          "mem64tb" : "memory = 64.TB",
          "mem128tb" : "memory = 128.TB",
          "mem256tb" : "memory = 256.TB",
          "mem512tb" : "memory = 512.TB",
          "cpu1" : "cpus = 1",
          "cpu2" : "cpus = 2",
          "cpu5" : "cpus = 5",
          "cpu10" : "cpus = 10",
          "cpu20" : "cpus = 20",
          "cpu50" : "cpus = 50",
          "cpu100" : "cpus = 100",
          "cpu200" : "cpus = 200",
          "cpu500" : "cpus = 500",
          "cpu1000" : "cpus = 1000"
        }
      },
      "debug" : false,
      "container" : "docker"
    }
  ],
  "info" : {
    "config" : "/home/nirmayi/data_intuitive/rnaseq.vsh/src/workflows/rnaseq/config.vsh.yaml",
    "platform" : "nextflow",
    "output" : "/home/nirmayi/data_intuitive/rnaseq.vsh/target/nextflow/workflows/rnaseq",
    "viash_version" : "0.8.0-RC5",
    "git_commit" : "efa0ffd2fbdce950e208caa1570584b258689789",
    "git_remote" : "https://github.com/data-intuitive/rnaseq.vsh.git"
  }
}'''))
]

// resolve dependencies dependencies (if any)
meta["root_dir"] = getRootDir()
include { prepare_genome } from "${meta.resources_dir}/../../../nextflow/workflows/prepare_genome/main.nf"
include { cat_fastq } from "${meta.resources_dir}/../../../nextflow/cat_fastq/main.nf"
include { pre_processing } from "${meta.resources_dir}/../../../nextflow/workflows/pre_processing/main.nf"
include { genome_alignment_and_quant } from "${meta.resources_dir}/../../../nextflow/workflows/genome_alignment_and_quant/main.nf"
include { post_processing } from "${meta.resources_dir}/../../../nextflow/workflows/post_processing/main.nf"
include { quality_control } from "${meta.resources_dir}/../../../nextflow/workflows/quality_control/main.nf"
include { salmon_quant_merge_counts } from "${meta.resources_dir}/../../../nextflow/workflows/salmon_quant_merge_counts/main.nf"

// inner workflow
// user-provided Nextflow code
workflow run_wf {
  take:
    input_ch

  main:
    analysis_ch = input_ch

    | map { id, state ->
      def biotype = state.gencode ? "gene_type" : state.featurecounts_group_type 
      [ id, state + [ biotype: biotype ] ]
    } 
    // | toSortedList
    // | map { list -> 
    //     [ list.collect{it[0]}.unique(),  
    //       [ fasta: list[1][-1].fasta,
    //       gtf: list[1][-1].gtf, 
    //       gff: list[1][-1].gff, 
    //       additional_fasta: list[1][-1].additional_fasta,
    //       transcript_fasta: list[1][-1].transcript_fasta, 
    //       gene_bed: list[1][-1].gene_bed,
    //       splicesites: list[1][-1].splicesites,
    //       bbsplit_fasta_list: list[1][-1].bbsplit_fasta_list,
    //       star_index: list[1][-1].star_index,
    //       rsem_index: list[1][-1].rsem_index,
    //       salmon_index: list[1][-1].salmon_index,
    //       hisat2_index: list[1][-1].hisat2_index,
    //       bbsplit_index: list[1][-1].bbsplit_index,
    //       gencode: list[1][-1].gencode,
    //       biotype: list[1][-1].biotype ]
    //     ]
    // } 
    // prepare all the necessary files for reference genome
    | prepare_genome.run ( 
        fromState: [
          "fasta": "fasta", 
          "gtf": "gtf", 
          "gff": "gff",
          "additional_fasta": "additional_fasta", 
          "transcript_fasta": "transcript_fasta", 
          "gene_bed": "gene_bed",
          // "splicesites": "splicesites",
          "bbsplit_fasta_list": "bbsplit_fasta_list", 
          "star_index": "star_index", 
          // "rsem_index": "rsem_index",
          "salmon_index": "salmon_index",
          // "hisat2_index": "hisat2_index",
          "bbsplit_index": "bbsplit_index",
          "gencode": "gencode", 
          "biotype": "biotype" 
        ],
        toState: [
          "fasta": "uncompressed_fasta", 
          "gtf": "gtf_uncompressed", 
          "transcript_fasta": "transcript_fasta_uncompressed", 
          "fai": "fai", 
          "chrom_sizes": "chrom_sizes", 
          "bbsplit_index": "bbsplit_index_uncompressed", 
          "star_index": "star_index_uncompressed", 
          "salmon_index": "salmon_index_uncompressed", 
          "gene_bed": "gene_bed_uncompressed" 
        ]
    )

    // Check if contigs in genome fasta file > 512 Mbp
    | map { id, state -> 
      (isBelowMaxContigSize(state.fai)) ? [id, state] : [id, state + [bam_csi_index: true]]
    }
    // | view

    // analysis_ch = input_ch
    // | combine(reference_ch)

    // Concatenate FastQ files from same sample if required
    | cat_fastq.run (
        fromState: [
          "read_1": "fastq_1", 
          "read_2": "fastq_2"
        ], 
        toState: [ 
          "fastq_1": "fastq_1",
          "fastq_2": "fastq_2"
        ]
    )
    
    // Pre-process fastq files
    | pre_processing.run ( 
        fromState: [
          "id": "id", 
          "fastq_1": "fastq_1",
          "fastq_2": "fastq_2", 
          "umitools_bc_pattern": "umitools_bc_pattern",
          "umitools_bc_pattern2": "umitools_bc_pattern2",
          "strandedness": "strandedness",
          "transcript_fasta": "transcript_fasta", 
          "gtf": "gtf",
          "with_umi": "with_umi", 
          "bbsplit_index": "bbsplit_index", 
          "bbsplit_fasta_list": "bbsplit_fasta_list", 
          "bc_pattern": "bc_pattern", 
          "ribo_database_manifest": "ribo_database_manifest", 
          "salmon_index": "salmon_index"
        ], 
        toState: [ 
          "fastqc_html_1": "fastqc_html_1",
          "fastqc_html_2": "fastqc_html_2",
          "fastqc_zip_1": "fastqc_zip_1",
          "fastqc_zip_2": "fastqc_zip_2",  
          "fastq_1": "qc_output1",
          "fastq_2": "qc_output2", 
          "trim_log_1": "trim_log_1", 
          "trim_log_2": "trim_log_2", 
          "trim_zip_1": "trim_zip_1",
          "trim_zip_2": "trim_zip_2",
          "trim_html_1": "trim_html_1",
          "trim_html_2": "trim_html_2",
          "passed_trimmed_reads": "passed_trimmed_reads",
          "num_trimmed_reads": "num_trimmed_reads",
          "sortmerna_log": "sortmerna_log",
          "salmon_json_info": "salmon_json_info"
        ]
    )

    // Infer strandedness from Salmon pseudo-alignment results
    | map { id, state -> 
    (state.strandedness == 'auto') ? 
      [ id, state + [strandedness: getSalmonInferredStrandedness(state.salmon_json_info)] ] : 
      [id, state] 
    }

    // Filter FastQ files based on minimum trimmed read count after adapter trimming
    | map { id, state -> 
      def input = state.fastq_2 ? [ state.fastq_1, state.fastq_2 ] : [ state.fastq_1 ]
      def num_reads = state.min_trimmed_reads + 1
      num_reads = 
        (!state.skip_trimming && input.size() == 2) ?
          getTrimGaloreReadsAfterFiltering(state.trim_log_2) : 
          getTrimGaloreReadsAfterFiltering(state.trim_log_1)
      def passed_trimmed_reads = 
        (state.skip_trimming || (num_reads >= state.min_trimmed_reads)) ? 
          true : 
          false 
      [ id, state + [num_trimmed_reads: num_reads, passed_trimmed_reads: passed_trimmed_reads] ] 
    }
    // | filter { id, state -> state.skip_trimming || state.passed_trimmed_reads }
    // TODO: Get list of samples that failed trimming threshold for MultiQC report

    // Genome alignment and quantification
    | genome_alignment_and_quant.run (
        runIf: { id, state -> state.passed_trimmed_reads },
        fromState: [
          "id": "id", 
          "fastq_1": "fastq_1",
          "fastq_2": "fastq_2", 
          "strandedness": "strandedness", 
          "gtf": "gtf",
          "transcript_fasta": "transcript_fasta",
          "bam_csi_index": "bam_csi_index", 
          "star_index": "star_index", 
          "extra_star_align_args": "extra_star_align_args", 
          "star_ignore_sjdbgtf": "star_ignore_sjdbgtf",
          "seq_platform": "seq_platform", 
          "seq_center": "seq_center",
          "with_umi": "with_umi", 
          "umi_dedup_stats": "umi_dedup_stats",
          "gtf_group_features": "gtf_group_features",
          "gtf_extra_attributes": "gtf_extra_attributes",
          "salmon_quant_libtype": "salmon_quant_libtype" 
        ],
        toState: [
          "star_alignment": "star_alignment", 
          "star_multiqc": "star_multiqc", 
          "genome_bam_sorted": "genome_bam_sorted",
          "genome_bam_index": "genome_bam_index", 
          "genome_bam_stats": "genome_bam_stats", 
          "genome_bam_flagstat": "genome_bam_flagstat", 
          "genome_bam_idxstats": "genome_bam_idxstats", 
          "transcriptome_bam_sorted": "transcriptome_bam_sorted", 
          "transcriptome_bam_index": "transcriptome_bam_index", 
          "transcriptome_bam_stats": "transcriptome_bam_stats", 
          "transcriptome_bam_flagstat": "transcriptome_bam_flagstat", 
          "transcriptome_bam_idxstats": "transcriptome_bam_idxstats",
          "salmon_quant_results": "salmon_quant_results"
        ]
    )

    // Filter channels to get samples that passed STAR minimum mapping percentage
    | map { id, state -> 
      def percent_mapped = getStarPercentMapped(state.star_multiqc) 
      def passed_mapping = (percent_mapped >= state.min_mapped_reads) ? true : false
      [ id, state + [percent_mapped: percent_mapped, passed_mapping: passed_mapping] ]
    }
    // | filter { id, state -> state.passed_mapping) }
    // TODO: Get list of samples that failed mapping for MultiQC report

    | map { id, state ->
      def input = state.fastq_2 ? [ state.fastq_1, state.fastq_2 ] : [ state.fastq_1 ]
      def paired = input.size() == 2
      [ id, state + [ paired: paired ] ]
    }
    // Post-processing
    | post_processing.run (
        runIf: { id, state -> state.passed_trimmed_reads && state.passed_mapping },
        fromState: [
          "id": "id", 
          "paired": "paired", 
          "strandedness": "strandedness", 
          "fasta": "fasta",
          "fai": "fai", 
          "gtf": "gtf", 
          "genome_bam": "genome_bam_sorted", 
          "chrom_sizes": "chrom_sizes", 
          "star_multiqc": "star_multiqc",
          "extra_picard_args": "extra_picard_args", 
          "extra_stringtie_args": "extra_stringtie_args", 
          "stringtie_ignore_gtf": "stringtie_ignore_gtf", 
          "extra_bedtools_args": "extra_bedtools_args", 
          "bam_csi_index": "bam_csi_index", 
          "min_mapped_reads": "min_mapped_reads", 
          "with_umi": "with_umi",
          "skip_qc": "skip_qc",
          "skip_markdupkicates": "skip_markdupkicates", 
          "skip_stringtie": "skip_stringtie", 
          "skip_bigwig":"gencode"
        ], 
        toState: [
          "genome_bam_sorted": "processed_genome_bam", 
          "genome_bam_index": "genome_bam_index",
          "genome_bam_stats": "genome_bam_stats",
          "genome_bam_flagstat": "genome_bam_flagstat", 
          "genome_bam_idxstats": "genome_bam_idxstats", 
          "markduplicates_metrics": "markduplicates_metrics",
          "stringtie_transcript_gtf": "stringtie_transcript_gtf",
          "stringtie_coverage_gtf": "stringtie_coverage_gtf",
          "stringtie_abundance": "stringtie_abundance",
          "stringtie_ballgown": "stringtie_ballgown", 
          "bedgraph_forward": "bedgraph_forward",
          "bedgraph_reverse": "bedgraph_reverse",
          "bigwig_forward": "bigwig_forward",
          "bigwig_reverse": "bigwig_reverse"
        ]
    )

    // Final QC
    | quality_control.run (
        fromState: [
          "id": "id", 
          "paired": "paired", 
          "strandedness": "strandedness", 
          "gtf": "gtf", 
          "genome_bam": "genome_bam_sorted", 
          "genome_bam_index": "genome_bam_index",
          "salmon_quant_results": "salmon_quant_results", 
          "gene_bed": "gene_bed",
          "extra_preseq_args": "extra_preseq_args",
          "extra_featurecounts_args": "extra_featurecounts_args", 
          "biotype": "biotype", 
          "biotypes_header": "biotypes_header",
          "skip_biotype_qc": "skip_biotype_qc", 
          "featurecounts_group_type": "featurecounts_group_type", 
          "featurecounts_feature_type": "featurecounts_feature_type", 
          "gencode": "gencode",
          "skip_deseq2_qc": "skip_deseq2_qc",  
          "extra_deseq2_args": "extra_deseq2_args",
          "extra_deseq2_args2": "extra_deseq2_args2",
          "multiqc_custom_config": "multiqc_custom_config", 
          "multiqc_title": "multiqc_title", 
          "multiqc_logo": "multiqc_logo",
          "multiqc_methods_description": "multiqc_methods_description",
          // "fail_trimming_multiqc": "fail_trimming_multiqc", 
          // "fail_mapping_multiqc": "fail_mapping_multiqc", 
          "fastqc_zip_1": "fastqc_zip_1",
          "fastqc_zip_2": "fastqc_zip_2",  
          "trim_log_1": "trim_log_1", 
          "trim_log_2": "trim_log_2", 
          "trim_zip_1": "trim_zip_1",
          "trim_zip_2": "trim_zip_2",
          "sortmerna_multiqc": "sortmerna_log", 
          "star_multiqc": "star_multiqc", 
          "genome_bam_stats": "genome_bam_stats", 
          "genome_bam_flagstat": "genome_bam_flagstat", 
          "genome_bam_idxstats": "genome_bam_idxstats", 
          "markduplicates_multiqc": "markduplicates_metrics", 
          "rseqc_modules": "rseqc_modules",
          "num_trimmed_reads": "num_trimmed_reads",
          "passed_trimmed_reads": "passed_trimmed_reads",
          "passed_mapping": "passed_mapping",
          "percent_mapped": "percent_mapped"
        ], 
        toState: [
          "preseq_output": "preseq_output",
          "bamstat_output": "bamstat_output",
          "strandedness_output": "strandedness_output",
          "inner_dist_output_stats": "inner_dist_output_stats",
          "inner_dist_output_dist": "inner_dist_output_dist",
          "inner_dist_output_freq": "inner_dist_output_freq",
          "inner_dist_output_plot": "inner_dist_output_plot",
          "inner_dist_output_plot_r": "inner_dist_output_plot_r",
          "junction_annotation_output_log": "junction_annotation_output_log",
          "junction_annotation_output_plot_r": "junction_annotation_output_plot_r",
          "junction_annotation_output_junction_bed": "junction_annotation_output_junction_bed",
          "junction_annotation_output_junction_interact": "junction_annotation_output_junction_interact",
          "junction_annotation_output_junction_sheet": "junction_annotation_output_junction_sheet",
          "junction_annotation_output_splice_events_plot": "junction_annotation_output_splice_events_plot",
          "junction_annotation_output_splice_junctions_plot": "junction_annotation_output_splice_junctions_plot",
          "junction_saturation_output_plot_r": "junction_saturation_output_plot_r",
          "junction_saturation_output_plot": "junction_saturation_output_plot",
          "read_distribution_output": "read_distribution_output",
          "read_duplication_output_duplication_rate_plot_r": "read_duplication_output_duplication_rate_plot_r",
          "read_duplication_output_duplication_rate_plot": "read_duplication_output_duplication_rate_plot",
          "read_duplication_output_duplication_rate_mapping": "read_duplication_output_duplication_rate_mapping",
          "read_duplication_output_duplication_rate_sequence": "read_duplication_output_duplication_rate_sequence",
          "tin_output_summary": "tin_output_summary",
          "tin_output_metrics": "tin_output_metrics",
          "dupradar_output_dupmatrix": "dupradar_output_dupmatrix",
          "dupradar_output_dup_intercept_mqc": "dupradar_output_dup_intercept_mqc",
          "dupradar_output_duprate_exp_boxplot": "dupradar_output_duprate_exp_boxplot",
          "dupradar_output_duprate_exp_densplot": "dupradar_output_duprate_exp_densplot",
          "dupradar_output_duprate_exp_denscurve_mqc": "dupradar_output_duprate_exp_denscurve_mqc",
          "dupradar_output_expression_histogram": "dupradar_output_expression_histogram",
          "dupradar_output_intercept_slope": "dupradar_output_intercept_slope",
          "qualimap_output_dir": "qualimap_output_dir",
          "qualimap_output_pdf": "qualimap_output_pdf",
          "featurecounts": "featurecounts",
          "featurecounts_summary": "featurecounts_summary"
        ] 
    )

    | map { id, state -> 
      def paired_state = (!state.paired) ? 
        [fastqc_html_2: state.remove(state.fastqc_html_2), fastqc_zip_2: state.remove(state.fastqc_zip_1), trim_log_2: state.remove(state.trim_log_2), trim_zip_2: state.remove(state.trim_zip_2), trim_html_2: state.remove(state.trim_html_2)] : 
        []
      def qc_state = (state.skip_qc || state.skip_fastqc) ? 
        [fastqc_html_1: state.remove(state.fastqc_html_1), fastqc_html_2: state.remove(state.fastqc_html_2), fastqc_zip_1: state.remove(state.fastqc_zip_1), fastqc_zip_2: state.remove(state.fastqc_zip_2)] : 
        []
      def trimming_state = (state.skip_trimming) ? 
        [trim_html_1: state.remove(state.trim_html_1), trim_html_2: state.remove(state.trim_html_2), trim_zip_1: state.remove(state.trim_zip_1), trim_zip_2: state.remove(state.trim_zip_2), trim_log_1: state.remove(state.trim_log_1), trim_log_2: state.remove(state.trim_log_2)] : 
        []
      def sortmerna_state = (!state.remove_ribo_rna) ? [sortmerna_log: state.remove(state.sortmerna_log)] : []
      [ id, state + paired_state + qc_state + trimming_state + sortmerna_state ]
    }
    
    | setState (
      [
        "output_fasta": "fasta", 
        "output_gtf": "gtf", 
        "output_transcript_fasta": "transcript_fasta", 
        "output_gene_bed": "gene_bed", 
        "output_bbsplit_index": "bbsplit_index", 
        "output_star_index": "star_index", 
        "output_salmon_index": "salmon_index", 
        "fastqc_html_1": "fastqc_html_1",
        "fastqc_html_2": "fastqc_html_2",
        "fastqc_zip_1": "fastqc_zip_1",
        "fastqc_zip_2": "fastqc_zip_2",  
        "output_fastq_1": "fastq_1",
        "output_fastq_2": "fastq_2", 
        "trim_log_1": "trim_log_1", 
        "trim_log_2": "trim_log_2", 
        "trim_zip_1": "trim_zip_1",
        "trim_zip_2": "trim_zip_2",
        "trim_html_1": "trim_html_1",
        "trim_html_2": "trim_html_2",
        "sortmerna_log": "sortmerna_log",
        "star_alignment": "star_alignment", 
        "genome_bam_sorted": "genome_bam_sorted",
        "genome_bam_index": "genome_bam_index", 
        "genome_bam_stats": "samtools_stats", 
        "genome_bam_flagstat": "samtools_flagstat", 
        "genome_bam_idxstats": "samtools_idxstats", 
        "transcriptome_bam_sorted": "transcriptome_bam_sorted", 
        "transcriptome_bam_index": "transcriptome_bam_index", 
        "transcriptome_bam_stats": "transcriptome_bam_stats", 
        "transcriptome_bam_flagstat": "transcriptome_bam_flagstat", 
        "transcriptome_bam_idxstats": "transcriptome_bam_idxstats",
        "salmon_quant_results": "salmon_quant_merged",
        "stringtie_transcript_gtf": "stringtie_transcript_gtf",
        "stringtie_coverage_gtf": "stringtie_coverage_gtf",
        "stringtie_abundance": "stringtie_abundance",
        "stringtie_ballgown": "stringtie_ballgown", 
        "featurecounts": "featurecounts",
        "featurecounts_summary": "featurecounts_summary", 
        "bedgraph_forward": "bedgraph_forward",
        "bedgraph_reverse": "bedgraph_reverse",
        "bigwig_forward": "bigwig_forward",
        "bigwig_reverse": "bigwig_reverse",
        "preseq_output": "preseq_output",
        "bamstat_output": "bamstat_output",
        "strandedness_output": "strandedness_output",
        "inner_dist_output_stats": "inner_dist_output_stats",
        "inner_dist_output_dist": "inner_dist_output_dist",
        "inner_dist_output_freq": "inner_dist_output_freq",
        "inner_dist_output_plot": "inner_dist_output_plot",
        "inner_dist_output_plot_r": "inner_dist_output_plot_r",
        "junction_annotation_output_log": "junction_annotation_output_log",
        "junction_annotation_output_plot_r": "junction_annotation_output_plot_r",
        "junction_annotation_output_junction_bed": "junction_annotation_output_junction_bed",
        "junction_annotation_output_junction_interact": "junction_annotation_output_junction_interact",
        "junction_annotation_output_junction_sheet": "junction_annotation_output_junction_sheet",
        "junction_annotation_output_splice_events_plot": "junction_annotation_output_splice_events_plot",
        "junction_annotation_output_splice_junctions_plot": "junction_annotation_output_splice_junctions_plot",
        "junction_saturation_output_plot_r": "junction_saturation_output_plot_r",
        "junction_saturation_output_plot": "junction_saturation_output_plot",
        "read_distribution_output": "read_distribution_output",
        "read_duplication_output_duplication_rate_plot_r": "read_duplication_output_duplication_rate_plot_r",
        "read_duplication_output_duplication_rate_plot": "read_duplication_output_duplication_rate_plot",
        "read_duplication_output_duplication_rate_mapping": "read_duplication_output_duplication_rate_mapping",
        "read_duplication_output_duplication_rate_sequence": "read_duplication_output_duplication_rate_sequence",
        "tin_output_summary": "tin_output_summary",
        "tin_output_metrics": "tin_output_metrics",
        "dupradar_output_dupmatrix": "dupradar_output_dupmatrix",
        "dupradar_output_dup_intercept_mqc": "dupradar_output_dup_intercept_mqc",
        "dupradar_output_duprate_exp_boxplot": "dupradar_output_duprate_exp_boxplot",
        "dupradar_output_duprate_exp_densplot": "dupradar_output_duprate_exp_densplot",
        "dupradar_output_duprate_exp_denscurve_mqc": "dupradar_output_duprate_exp_denscurve_mqc",
        "dupradar_output_expression_histogram": "dupradar_output_expression_histogram",
        "dupradar_output_intercept_slope": "dupradar_output_intercept_slope",
        "qualimap_output_dir": "qualimap_output_dir",
        // "qualimap_output_pdf": "qualimap_output_pdf" 
      ]
    )

    // | niceView()

    output_ch = analysis_ch


  emit:
    output_ch
}

import nextflow.Nextflow
//
// Function to generate an error if contigs in genome fasta file > 512 Mbp
//
def isBelowMaxContigSize(fai_file) {
  def max_size = 512000000
  fai_file.eachLine { line ->
    def lspl  = line.split('\t')
    def chrom = lspl[0]
    def size  = lspl[1]
    if (size.toInteger() > max_size) {
      def error_string = "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n" +
        "  Contig longer than ${max_size}bp found in reference genome!\n\n" +
        "  ${chrom}: ${size}\n\n" +
        "  Provide the '--bam_csi_index' parameter to use a CSI instead of BAI index.\n\n" +
        "  Please see:\n" +
        "  https://github.com/nf-core/rnaseq/issues/744\n" +
        "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"
      Nextflow.error(error_string)
      return false
    }
  }
  return true
}

import groovy.json.JsonSlurper
//
// Function that parses Salmon quant 'meta_info.json' output file to get inferred strandedness
//
def getSalmonInferredStrandedness(json_file) {
  def lib_type = new JsonSlurper().parseText(json_file.text).get('library_types')[0]
  def strandedness = 'reverse'
  if (lib_type) {
    if (lib_type in ['U', 'IU']) {
      strandedness = 'unstranded'
    } 
    else if (lib_type in ['SF', 'ISF']) {
      strandedness = 'forward'
    } 
    else if (lib_type in ['SR', 'ISR']) {
      strandedness = 'reverse'
    }
  }
  return strandedness
}

//
// Function that parses TrimGalore log output file to get total number of reads after trimming
//
def getTrimGaloreReadsAfterFiltering(log_file) {
  def total_reads = 0
  def filtered_reads = 0
  log_file.eachLine { line ->
    def total_reads_matcher = line =~ /([\d\.]+)\ssequences processed in total/
    def filtered_reads_matcher = line =~ /shorter than the length cutoff[^:]+:\s([\d\.]+)/
    if (total_reads_matcher) total_reads = total_reads_matcher[0][1].toFloat()
    if (filtered_reads_matcher) filtered_reads = filtered_reads_matcher[0][1].toFloat()
  }
  return total_reads - filtered_reads
}

//
// Function that parses and returns the alignment rate from the STAR log output
//
def getStarPercentMapped(align_log) {
  def percent_aligned = 0
  def pattern = /Uniquely mapped reads %\s*\|\s*([\d\.]+)%/
  align_log.eachLine { line ->
    def matcher = line =~ pattern
    if (matcher) {
        percent_aligned = matcher[0][1].toFloat()
    }
  }
  return percent_aligned
}

//
// Create MultiQC tsv custom content from a list of values
//
def multiqcTsvFromList(tsv_data, header) {
  def tsv_string = ""
  if (tsv_data.size() > 0) {
    tsv_string += "${header.join('\t')}\n"
    tsv_string += tsv_data.join('\n')
  }
  return tsv_string
}

// inner workflow hook
def innerWorkflowFactory(args) {
  return run_wf
}

// defaults
meta["defaults"] = [
  // key to be used to trace the process and determine output names
  key: null,

  // fixed arguments to be passed to script
  args: [:],

  // default directives
  directives: readJsonBlob('''{
  "tag" : "$id"
}'''),

  // auto settings
  auto: readJsonBlob('''{
  "simplifyInput" : true,
  "simplifyOutput" : false,
  "transcript" : false,
  "publish" : false
}'''),

  // Apply a map over the incoming tuple
  // Example: `{ tup -> [ tup[0], [input: tup[1].output] ] + tup.drop(2) }`
  map: null,

  // Apply a map over the ID element of a tuple (i.e. the first element)
  // Example: `{ id -> id + "_foo" }`
  mapId: null,

  // Apply a map over the data element of a tuple (i.e. the second element)
  // Example: `{ data -> [ input: data.output ] }`
  mapData: null,

  // Apply a map over the passthrough elements of a tuple (i.e. the tuple excl. the first two elements)
  // Example: `{ pt -> pt.drop(1) }`
  mapPassthrough: null,

  // Filter the channel
  // Example: `{ tup -> tup[0] == "foo" }`
  filter: null,

  // Choose whether or not to run the component on the tuple if the condition is true.
  // Otherwise, the tuple will be passed through.
  // Example: `{ tup -> tup[0] != "skip_this" }`
  runIf: null,

  // Rename keys in the data field of the tuple (i.e. the second element)
  // Will likely be deprecated in favour of `fromState`.
  // Example: `[ "new_key": "old_key" ]`
  renameKeys: null,

  // Fetch data from the state and pass it to the module without altering the current state.
  // 
  // `fromState` should be `null`, `List[String]`, `Map[String, String]` or a function. 
  // 
  // - If it is `null`, the state will be passed to the module as is.
  // - If it is a `List[String]`, the data will be the values of the state at the given keys.
  // - If it is a `Map[String, String]`, the data will be the values of the state at the given keys, with the keys renamed according to the map.
  // - If it is a function, the tuple (`[id, state]`) in the channel will be passed to the function, and the result will be used as the data.
  // 
  // Example: `{ id, state -> [input: state.fastq_file] }`
  // Default: `null`
  fromState: null,

  // Determine how the state should be updated after the module has been run.
  // 
  // `toState` should be `null`, `List[String]`, `Map[String, String]` or a function.
  // 
  // - If it is `null`, the state will be replaced with the output of the module.
  // - If it is a `List[String]`, the state will be updated with the values of the data at the given keys.
  // - If it is a `Map[String, String]`, the state will be updated with the values of the data at the given keys, with the keys renamed according to the map.
  // - If it is a function, a tuple (`[id, output, state]`) will be passed to the function, and the result will be used as the new state.
  //
  // Example: `{ id, output, state -> state + [counts: state.output] }`
  // Default: `{ id, output, state -> output }`
  toState: null,

  // Whether or not to print debug messages
  // Default: `false`
  debug: false
]

// initialise default workflow
meta["workflow"] = workflowFactory([key: meta.config.functionality.name], meta.defaults, meta)

// add workflow to environment
nextflow.script.ScriptMeta.current().addDefinition(meta.workflow)

// anonymous workflow for running this module as a standalone
workflow {
  // add id argument if it's not already in the config
  // TODO: deep copy
  def newConfig = deepClone(meta.config)
  def newParams = deepClone(params)

  def argsContainsId = newConfig.functionality.allArguments.any{it.plainName == "id"}
  if (!argsContainsId) {
    def idArg = [
      'name': '--id',
      'required': false,
      'type': 'string',
      'description': 'A unique id for every entry.',
      'multiple': false
    ]
    newConfig.functionality.arguments.add(0, idArg)
    newConfig = processConfig(newConfig)
  }
  if (!newParams.containsKey("id")) {
    newParams.id = "run"
  }

  helpMessage(newConfig)

  channelFromParams(newParams, newConfig)
    // make sure id is not in the state if id is not in the args
    | map {id, state ->
      if (!argsContainsId) {
        [id, state.findAll{k, v -> k != "id"}]
      } else {
        [id, state]
      }
    }
    | meta.workflow.run(
      auto: [ publish: "state" ]
    )
}

// END COMPONENT-SPECIFIC CODE
